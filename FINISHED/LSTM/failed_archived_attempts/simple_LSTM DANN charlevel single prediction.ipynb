{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "from tokenizers import CharBPETokenizer\n",
    "import functools\n",
    "import time\n",
    "\n",
    "\n",
    "gpu_device = jax.device_get('gpu')[0]\n",
    "cpu_device = jax.device_get('cpu')[0]\n",
    "# LSTM\n",
    "# xs = B, input_size = B, T, C\n",
    "# h = c = y = B, output_size = B, T, logits_size = B, T, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 155\n",
      "dog [66 77 69] dog\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "with open('data/dnbt_posts.txt', 'r') as file:\n",
    "  dataset = file.read()\n",
    "\n",
    "# tokenize\n",
    "vocab = sorted(list(set(dataset)))\n",
    "print(\"vocab length:\", len(vocab))\n",
    "\n",
    "token_to_char = dict(enumerate(vocab))\n",
    "char_to_token = dict([(v, k) for k, v in token_to_char.items()])\n",
    "decode = lambda tokens: \"\".join([token_to_char[int(token)] for token in tokens])\n",
    "encode = lambda chars: jnp.array([char_to_token[c] for c in chars])\n",
    "\n",
    "print(\"dog\", encode(\"dog\"), decode(encode(\"dog\")))\n",
    "\n",
    "dataset_tokens = encode(dataset)\n",
    "split_ratio = 0.8\n",
    "train_tokens = dataset_tokens[:int(len(dataset_tokens)*split_ratio)]\n",
    "test_tokens = dataset_tokens[int(len(dataset_tokens)*split_ratio):]\n",
    "del dataset\n",
    "del dataset_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layers = 1\n",
    "sequence_length = 25# 100\n",
    "model_size = 64# 512\n",
    "\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "\n",
    "\n",
    "# init LSTM params\n",
    "def init_LSTM_params(key, lstm_layers, input_size, model_size, output_size):\n",
    "  param_sets = 8 # manual, idc\n",
    "  keys = random.split(key, param_sets*lstm_layers + 2)\n",
    "  hxconcat_size = model_size + model_size\n",
    "  he = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / shape[0])\n",
    "  # supposedly xavier is better for networks using tanh\n",
    "  xavier = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / (shape[0] + shape[1]))\n",
    "  params = [\n",
    "    {\n",
    "      \"wU\" : xavier(keys[param_sets*i + 0], (hxconcat_size, model_size)),\n",
    "      \"bU\" : jnp.zeros((model_size,)),\n",
    "      \"wC\" : xavier(keys[param_sets*i + 6], (hxconcat_size, model_size)),\n",
    "      \"bC\" : jnp.zeros((model_size,)),\n",
    "      \"wF1\": xavier(keys[param_sets*i + 1], (hxconcat_size, model_size)),\n",
    "      \"bF1\": jnp.zeros((model_size,)),\n",
    "      \"wF2\": xavier(keys[param_sets*i + 2], (hxconcat_size, model_size)),\n",
    "      \"bF2\": jnp.zeros((model_size,)),\n",
    "      \"wO\" : xavier(keys[param_sets*i + 3], (hxconcat_size, model_size)),\n",
    "      \"bO\" : jnp.zeros((model_size,)),\n",
    "      \"h0\" : jnp.zeros((model_size,)),\n",
    "      \"c0\" : jnp.zeros((model_size,)),\n",
    "      #\"h0\" : random.normal(keys[param_sets*i + 4], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "      #\"c0\" : random.normal(keys[param_sets*i + 5], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "    }\n",
    "    for i in range(lstm_layers)\n",
    "  ]\n",
    "  params[0].update(\n",
    "    {\n",
    "    # then embedding table weight and bias\n",
    "    \"wEM\" : he(keys[param_sets*(param_sets - 1) + 2], (input_size, model_size)),\n",
    "    \"bEM\" : jnp.zeros((model_size,)),\n",
    "\n",
    "  })\n",
    "  params[-1].update(\n",
    "    {\n",
    "      # this is for the y layer, which i am probably imlementing wrong.\n",
    "      \"wY1\" : he(keys[param_sets*(lstm_layers-1) + 4], (model_size, output_size)),\n",
    "      \"bY1\" : jnp.zeros((output_size,)),\n",
    "    }\n",
    "  )\n",
    "  return params\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def dropout(dropout_key, original_tensor, dropout_rate):\n",
    "  # generate random of same shape\n",
    "  dropout_probs = random.uniform(dropout_key, shape=original_tensor.shape)\n",
    "  # mask = random < dropout_rate\n",
    "  mask = (dropout_probs > dropout_rate) / (1 - dropout_rate) # scale to keep avg the same\n",
    "  return original_tensor * mask\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def lstm_step(step_dropout_key, lstm_layer_params, layer_h, layer_c, current_xt, dropout_rate):\n",
    "  hxconcat = jax.lax.concatenate([layer_h, current_xt], dimension=1) #B, h ++ B, C => B, h+c\n",
    "  # update gate\n",
    "  update = jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wU\"] + lstm_layer_params[\"bU\"])\n",
    "  #update = dropout(step_dropout_keys[0], update, dropout_rate)\n",
    "  candidate = jax.nn.tanh(hxconcat @ lstm_layer_params[\"wC\"] + lstm_layer_params[\"bC\"])\n",
    "  #candidate = dropout(step_dropout_keys[1], candidate, dropout_rate)\n",
    "\n",
    "  # forget gate\n",
    "  forget = jax.nn.sigmoid(\n",
    "              hxconcat @ lstm_layer_params[\"wF1\"] + lstm_layer_params[\"bF1\"]\n",
    "            ) * jax.nn.tanh(\n",
    "              hxconcat @ lstm_layer_params[\"wF2\"] + lstm_layer_params[\"bF2\"]\n",
    "            )\n",
    "\n",
    "  # update c with update and forget\n",
    "  layer_c = layer_c + update * candidate + forget # (batch, c) => (batch, c)\n",
    "\n",
    "  # output\n",
    "  layer_h = jax.nn.tanh(layer_c) * jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wO\"] + lstm_layer_params[\"bO\"]) # (B, model_size)\n",
    "\n",
    "  next_layer_xt = dropout(step_dropout_key, layer_h, dropout_rate) # the next layer's input x is the current layer's hidden state\n",
    "  # karpathy: dropout after EACH LAYER not several times in the block. lol.\n",
    "\n",
    "  return (layer_h, layer_c), next_layer_xt\n",
    "\n",
    "\n",
    "# LSTM forward\n",
    "import functools\n",
    "@functools.partial(jax.jit, static_argnames=['dropout_rate', 'lstm_layers'])\n",
    "def lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate, lstm_layers=lstm_layers):\n",
    "  batches = xembeds_batch.shape[0]\n",
    "  lstm_layers = len(lstm_params)\n",
    "  # initialize h and c as random/learnable params\n",
    "  #h = jnp.tile(lstm_params[0][\"h0\"], (batches, lstm_layers, 1)) # B, lstm_layer, h_size\n",
    "  #c = jnp.tile(lstm_params[0][\"c0\"], (batches, lstm_layers, 1)) # B, lstm_layer, c_size\n",
    "  # wait.. these are the same for all of the layers.. maybe they shouldn't be\n",
    "  T = xembeds_batch.shape[1]\n",
    "  # take xembeds_batch and pass each xt through the same SINGULAR block. don't update the weight layer. there is only one layer.\n",
    "  dropout_keys = random.split(dropout_key, lstm_layers)\n",
    "\n",
    "  # for each layer:\n",
    "    # scan over xt\n",
    "    # carry : h, c\n",
    "    # a: xt\n",
    "    # b: h,c\n",
    "    # f = lambda ((h, c), xt) : lstm_step(h, c, xt, everything else) => h, c\n",
    "    # scans over xt\n",
    "    # for next layer: xt = h of previous layer. h = h0 and c = c0\n",
    "  \n",
    "  current_embeddings_batch = jnp.transpose(xembeds_batch, (1, 0, 2)) # B, T, C => T, B, C\n",
    "    # The reason for this is that jax.lax.scan only uses the leading dim. why? idk. its dumb, it needs an axis arg so i can scan over whatever\n",
    "\n",
    "  for lstm_layer in range(lstm_layers):\n",
    "    h = jnp.tile(lstm_params[lstm_layer][\"h0\"], (batches, 1))\n",
    "    c = jnp.tile(lstm_params[lstm_layer][\"c0\"], (batches, 1))\n",
    "    layer_dropout_key = dropout_keys[lstm_layer] # it doesnt matter if this is the same across all layers\n",
    "    # scan should be inexpensive since layer size is small while t size is usually LARGE\n",
    "    # scan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\n",
    "    # scan :: scanfunc -> h_and_c -> xs -> (h_and_c_final, hs_to_be_used_as_input_xt_in_next_layer)\n",
    "    # scanfunc :: (c -> a -> (c, b))\n",
    "    scanfunc = lambda hc, xt : lstm_step(layer_dropout_key, lstm_params[lstm_layer], hc[0], hc[1], xt, dropout_rate)\n",
    "      # for xs: scan along the t dimension! it scans along B by default\n",
    "      # to fix this, we transpose xs with jnp.transpose(current_embeddings_batch, (1, 0, 2))\n",
    "    current_embeddings_batch = jax.lax.scan(scanfunc, (h, c), current_embeddings_batch)[1] # (c, [b]) => [b] ==> B, T, C\n",
    "  \n",
    "\n",
    "  # finally turn current_embeddings_batch into ys (logits)\n",
    "  hs = jnp.transpose(current_embeddings_batch, (1, 0, 2)) # T, B, C => B, T, C\n",
    "  ys = hs @ lstm_params[-1]['wY1'] + lstm_params[-1][\"bY1\"] # B, T, model_size => B, T, vocab_size\n",
    "  return ys\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def final_token_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch) # (B, T, C)\n",
    "  logit = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)[:, -1] # get last logit in each B. (B, vocab_size)\n",
    "  vocab_size = logit.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1) # get last y (the target). (B, vocab_size)\n",
    "  logprobs = jax.nn.log_softmax(logit, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1) # (B, vocab_size) => (B,)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses) # num\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "\n",
    "jitted_backwards_loss = jax.jit(jax.value_and_grad(final_token_loss, argnums=1), static_argnames=[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['vocab_size'])\n",
    "def embed(lstm_params, xtokens, vocab_size=len(vocab)):\n",
    "  xs_one_hot = jax.nn.one_hot(xtokens, vocab_size, axis=-1) #B, T, vocab_size\n",
    "  activations = xs_one_hot @ lstm_params[0][\"wEM\"] + lstm_params[0][\"bEM\"]\n",
    "  return activations\n",
    "\n",
    "\n",
    "lr = 1e-1\n",
    "lr_decay = 0.97\n",
    "decay_after = 10\n",
    "decay_every = 5\n",
    "optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "\n",
    "\n",
    "# make optimizer a static arg in jit or it breaks\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate):\n",
    "  step_loss, grads = jitted_backwards_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, lstm_params)\n",
    "  updated_lstm_params = optax.apply_updates(lstm_params, param_updates) \n",
    "  return updated_lstm_params, updated_opt_state, step_loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (1) | \"ke your own shakes w yogur\"\n",
      "PRED   (1) | \"ke your own shakes w yogut\"\n",
      "step (0, 49) || samples/sec: 1974 || loss: 3.4018 || val_loss: 3.4366 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (0, 50) || samples/sec: 1169437 || loss: 3.3992 || val_loss: 2.9033 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (0, 100) || samples/sec: 29459 || loss: 3.2284 || val_loss: 2.6206 val_acc: 0.2000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (0, 150) || samples/sec: 26271 || loss: 3.1477 || val_loss: 3.3975 val_acc: 0.0667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (1, 191) || samples/sec: 32535 || loss: 3.1201 || val_loss: 2.6407 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (1, 211) || samples/sec: 70701 || loss: 3.1065 || val_loss: 2.7552 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (1, 261) || samples/sec: 29920 || loss: 3.0743 || val_loss: 2.7255 val_acc: 0.2000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (1, 311) || samples/sec: 28975 || loss: 3.0528 || val_loss: 3.3022 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (2, 352) || samples/sec: 34174 || loss: 3.0479 || val_loss: 2.6658 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (2, 372) || samples/sec: 69027 || loss: 3.0452 || val_loss: 2.7223 val_acc: 0.4000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (2, 422) || samples/sec: 29109 || loss: 3.0372 || val_loss: 2.6133 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imple\"\n",
      "step (2, 472) || samples/sec: 29524 || loss: 3.0305 || val_loss: 3.2127 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other n\"\n",
      "step (3, 513) || samples/sec: 36938 || loss: 3.0304 || val_loss: 2.7242 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      ret\"\n",
      "step (3, 533) || samples/sec: 70574 || loss: 3.0295 || val_loss: 2.7004 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (3, 583) || samples/sec: 29809 || loss: 3.0258 || val_loss: 2.5706 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imple\"\n",
      "step (3, 633) || samples/sec: 29278 || loss: 3.0243 || val_loss: 3.3488 val_acc: 0.2000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other  \"\n",
      "step (4, 674) || samples/sec: 36218 || loss: 3.0276 || val_loss: 2.8095 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (4, 694) || samples/sec: 71558 || loss: 3.0296 || val_loss: 3.3990 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (4, 744) || samples/sec: 29820 || loss: 3.0351 || val_loss: 2.5697 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imple\"\n",
      "step (4, 794) || samples/sec: 29607 || loss: 3.0396 || val_loss: 3.3925 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (5, 835) || samples/sec: 36288 || loss: 3.0507 || val_loss: 2.7782 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "step (5, 855) || samples/sec: 76371 || loss: 3.0583 || val_loss: 3.7534 val_acc: 0.1333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it wase\"\n",
      "step (5, 905) || samples/sec: 29358 || loss: 3.0724 || val_loss: 2.6763 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (5, 955) || samples/sec: 29232 || loss: 3.0800 || val_loss: 3.4175 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (6, 996) || samples/sec: 36520 || loss: 3.0879 || val_loss: 2.8895 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (6, 1016) || samples/sec: 70645 || loss: 3.0936 || val_loss: 4.2371 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (6, 1066) || samples/sec: 29174 || loss: 3.0995 || val_loss: 2.6635 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (6, 1116) || samples/sec: 29481 || loss: 3.0997 || val_loss: 3.5982 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (7, 1157) || samples/sec: 34435 || loss: 3.0998 || val_loss: 2.8704 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (7, 1177) || samples/sec: 70467 || loss: 3.1016 || val_loss: 3.8072 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (7, 1227) || samples/sec: 28851 || loss: 3.0985 || val_loss: 2.5470 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (7, 1277) || samples/sec: 27820 || loss: 3.0962 || val_loss: 3.6734 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (8, 1318) || samples/sec: 33262 || loss: 3.0955 || val_loss: 2.8161 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (8, 1338) || samples/sec: 73229 || loss: 3.0967 || val_loss: 3.0505 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (8, 1388) || samples/sec: 29268 || loss: 3.0945 || val_loss: 2.4190 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (8, 1438) || samples/sec: 28960 || loss: 3.0930 || val_loss: 3.9096 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (9, 1479) || samples/sec: 35002 || loss: 3.0949 || val_loss: 2.7103 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (9, 1499) || samples/sec: 75674 || loss: 3.0967 || val_loss: 3.1206 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (9, 1549) || samples/sec: 29632 || loss: 3.0972 || val_loss: 2.6499 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (9, 1599) || samples/sec: 28910 || loss: 3.0959 || val_loss: 3.6507 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (10, 1640) || samples/sec: 35465 || loss: 3.0973 || val_loss: 2.7068 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (10, 1660) || samples/sec: 74643 || loss: 3.0983 || val_loss: 3.1394 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (10, 1710) || samples/sec: 29539 || loss: 3.0974 || val_loss: 2.6178 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (10, 1760) || samples/sec: 29803 || loss: 3.0966 || val_loss: 3.5342 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (11, 1801) || samples/sec: 36298 || loss: 3.0970 || val_loss: 2.7568 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (11, 1821) || samples/sec: 71420 || loss: 3.0978 || val_loss: 3.0978 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (11, 1871) || samples/sec: 30814 || loss: 3.0963 || val_loss: 2.6249 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (11, 1921) || samples/sec: 30505 || loss: 3.0948 || val_loss: 3.4750 val_acc: 0.1333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (12, 1962) || samples/sec: 38693 || loss: 3.0948 || val_loss: 2.7652 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (12, 1982) || samples/sec: 73197 || loss: 3.0954 || val_loss: 2.9938 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (12, 2032) || samples/sec: 30529 || loss: 3.0942 || val_loss: 2.6437 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (12, 2082) || samples/sec: 30083 || loss: 3.0916 || val_loss: 3.3476 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (13, 2123) || samples/sec: 37146 || loss: 3.0915 || val_loss: 2.8107 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (13, 2143) || samples/sec: 72209 || loss: 3.0914 || val_loss: 2.9380 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (13, 2193) || samples/sec: 30632 || loss: 3.0894 || val_loss: 2.6479 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (13, 2243) || samples/sec: 25184 || loss: 3.0878 || val_loss: 3.4418 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (14, 2284) || samples/sec: 35891 || loss: 3.0872 || val_loss: 2.8002 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (14, 2304) || samples/sec: 68129 || loss: 3.0869 || val_loss: 2.9328 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (14, 2354) || samples/sec: 29208 || loss: 3.0848 || val_loss: 2.5872 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (14, 2404) || samples/sec: 29203 || loss: 3.0826 || val_loss: 3.4236 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (15, 2445) || samples/sec: 2566 || loss: 3.0816 || val_loss: 2.7938 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (15, 2465) || samples/sec: 70566 || loss: 3.0812 || val_loss: 2.8681 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (15, 2515) || samples/sec: 28426 || loss: 3.0800 || val_loss: 2.5680 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (15, 2565) || samples/sec: 28779 || loss: 3.0783 || val_loss: 3.4831 val_acc: 0.0333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (16, 2606) || samples/sec: 33572 || loss: 3.0783 || val_loss: 2.8231 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (16, 2626) || samples/sec: 66550 || loss: 3.0784 || val_loss: 3.1317 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (16, 2676) || samples/sec: 28743 || loss: 3.0770 || val_loss: 2.5653 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (16, 2726) || samples/sec: 28844 || loss: 3.0757 || val_loss: 3.5327 val_acc: 0.0333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (17, 2767) || samples/sec: 35752 || loss: 3.0760 || val_loss: 2.8053 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (17, 2787) || samples/sec: 71890 || loss: 3.0764 || val_loss: 3.0149 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (17, 2837) || samples/sec: 29437 || loss: 3.0755 || val_loss: 2.5622 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (17, 2887) || samples/sec: 28634 || loss: 3.0741 || val_loss: 3.5414 val_acc: 0.0333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (18, 2928) || samples/sec: 35964 || loss: 3.0739 || val_loss: 2.8060 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (18, 2948) || samples/sec: 69854 || loss: 3.0740 || val_loss: 3.3283 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (18, 2998) || samples/sec: 29293 || loss: 3.0729 || val_loss: 2.5495 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (18, 3048) || samples/sec: 28856 || loss: 3.0717 || val_loss: 3.5239 val_acc: 0.1000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (19, 3089) || samples/sec: 33381 || loss: 3.0721 || val_loss: 2.7998 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (19, 3109) || samples/sec: 66816 || loss: 3.0723 || val_loss: 2.9394 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (19, 3159) || samples/sec: 25411 || loss: 3.0713 || val_loss: 2.5412 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (19, 3209) || samples/sec: 27652 || loss: 3.0704 || val_loss: 3.5477 val_acc: 0.0667 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (20, 3250) || samples/sec: 35599 || loss: 3.0703 || val_loss: 2.8137 val_acc: 0.3333 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (20, 3270) || samples/sec: 69281 || loss: 3.0703 || val_loss: 2.9438 val_acc: 0.3000 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (20, 3320) || samples/sec: 29953 || loss: 3.0691 || val_loss: 2.5307 val_acc: 0.3333 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (20, 3370) || samples/sec: 30284 || loss: 3.0680 || val_loss: 3.5338 val_acc: 0.0667 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (21, 3411) || samples/sec: 34985 || loss: 3.0681 || val_loss: 2.8074 val_acc: 0.3333 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" in the hamptonsðŸ›‘      rep\"\n",
      "PRED   (1) | \" in the hamptonsðŸ›‘      re \"\n",
      "step (21, 3431) || samples/sec: 73897 || loss: 3.0682 || val_loss: 3.2990 val_acc: 0.3000 || LR = 0.094090\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m train_data_idx \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[1;32m     31\u001b[0m next_train_data_idx \u001b[38;5;241m=\u001b[39m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[0;32m---> 32\u001b[0m xtokens_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnext_train_data_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#(B, T)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m ytokens_batch \u001b[38;5;241m=\u001b[39m train_tokens[train_data_idx\u001b[38;5;241m+\u001b[39msequence_length:next_train_data_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:sequence_length] \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m dropout_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(epoch\u001b[38;5;241m*\u001b[39msteps \u001b[38;5;241m+\u001b[39m i) \u001b[38;5;66;03m# unique for every step\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:1249\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(operand, new_sizes, dimensions)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m   dyn_shape, static_new_sizes \u001b[38;5;241m=\u001b[39m _extract_tracers_dyn_shape(new_sizes)\n\u001b[0;32m-> 1249\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreshape_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstatic_new_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msame_dims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/core.py:438\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    436\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    437\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/core.py:442\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    441\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 442\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/core.py:955\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    953\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:91\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     89\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "# set up lstm params\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "opt_state = optimizer.init(lstm_params)\n",
    "\n",
    "\n",
    "# train\n",
    "# for now just overfit on small sample idk lol\n",
    "train_batch_size = 100\n",
    "val_batch_size = 30\n",
    "\n",
    "dropout_rate = 0\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "print_every = 50\n",
    "j = 0\n",
    "losses = []\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "  if epoch > decay_after:\n",
    "    if epoch % decay_every == 0:\n",
    "      lr *= lr_decay\n",
    "      opt_state.hyperparams['learning_rate'] = lr\n",
    "  steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "  for i in range(steps): # probably wrong but w/e\n",
    "    # train\n",
    "    # B, T where T = sequence_length\n",
    "    train_data_idx = i*sequence_length*train_batch_size\n",
    "    next_train_data_idx = (i+1)*sequence_length*train_batch_size\n",
    "    xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "    ytokens_batch = train_tokens[train_data_idx+sequence_length:next_train_data_idx+1:sequence_length] # (B,)\n",
    "\n",
    "    dropout_key = random.PRNGKey(epoch*steps + i) # unique for every step\n",
    "\n",
    "    lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate)\n",
    "\n",
    "    j += 1\n",
    "    losses.append(step_loss)\n",
    "\n",
    "    if j % print_every == 0:\n",
    "      end = time.time()\n",
    "      duration = end - start\n",
    "      # train inference example (no dropout)\n",
    "      xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "      last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0)[:, -1] # B, C\n",
    "      prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "      # val batch\n",
    "      j = i % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      val_idx = j*val_batch_size*sequence_length\n",
    "      next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[val_idx + sequence_length:next_val_idx+1:sequence_length]\n",
    "      xembeds_val_batch = embed(lstm_params, xtokens_val_batch)\n",
    "      \n",
    "      last_logits_val_batch = lstm_forward(dropout_key, lstm_params, xembeds_val_batch, 0)[:, -1] # (B, C)\n",
    "      prediction_val_batch = jnp.argmax(last_logits_val_batch, axis=-1) # (B,)\n",
    "      ys_onehot = jax.nn.one_hot(ytokens_val_batch, len(vocab), axis=-1) # (B, vocab_size)\n",
    "      logprobs = jax.nn.log_softmax(last_logits_val_batch, axis=-1) # (B, vocab_size)\n",
    "      crossentropies = -jnp.sum(ys_onehot*logprobs,axis=-1) # (B,)\n",
    "      val_loss = jnp.mean(crossentropies) # num\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "      y = decode([ytokens_batch[0]]).replace('\\n', ' ')\n",
    "      yhat = decode([prediction_batch[0]]).replace('\\n', ' ')\n",
    "      #print(epoch, epoch * samples + i, f\"{step_loss:1.4f}\", \"pred:\", x, \"=>\", y, \"?=\", yhat)\n",
    "      print(f'TARGET ({len(y)}) | \"{x}{y}\"')\n",
    "      print(f'PRED   ({len(yhat)}) | \"{x}{yhat}\"')\n",
    "      print(f\"step {(epoch, epoch * steps + i)} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || loss: {sum(losses)/len(losses):1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || LR = {opt_state.hyperparams['learning_rate']:0.6f}\" )\n",
    "      print()\n",
    "      start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reply: @APIGuy forer eving being x youre at they theursing this they for mudy hint a staully\n",
      "\n",
      "Ave word wookin on stapface in'at what ban\n",
      "I fearny https://t.co/pLi&LHLRED oh an @cenderenters into taybey really of dato i to some googing code bagel id of this cerast you siwle\n",
      "\n",
      "LLe too fis hex mistyress tleed liter interestings Coive to, hrave to by what bothing stuch (â€¦ monsing, to coding, hofrul https://t.co/kuILDhtnB bnigh caring the wress you hock stulf more they they this and llikeðŸ›‘"
     ]
    }
   ],
   "source": [
    "def inference(key, chars):\n",
    "  xtokens = encode(chars)[None, :]\n",
    "  xembed = embed(lstm_params, xtokens) # artificial single batch\n",
    "  logits = lstm_forward(key, lstm_params, xembed, 0)[0][-1] # logits of the first B and last T in the B T C. should be (C,)\n",
    "  yhattokens = random.choice(key, a=logits.shape[0], p=jax.nn.softmax(logits)) # no need for axis=-1 since logits are (C,)\n",
    "  sequence = yhattokens\n",
    "  return sequence\n",
    "\n",
    "steps = 1000\n",
    "import time\n",
    "seed = int(time.time())\n",
    "keys = random.split(random.PRNGKey(seed), steps)\n",
    "temperature = 0.5\n",
    "text =  \"\\n\"*50 + 'reply: @APIGuy'\n",
    "print(text.replace('\\n\\n', ''), end='')\n",
    "for i in range(steps):\n",
    "  next_token = inference(keys[i], text[-sequence_length:])\n",
    "  next_char = decode([next_token])[-1]\n",
    "  if next_char == 'ðŸ›‘':\n",
    "    print(next_char, end='')\n",
    "    break\n",
    "  text += next_char\n",
    "  print(next_char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bC': Array(1.4255815e-07, dtype=float32),\n",
       "  'bEM': Array(0.0001805, dtype=float32),\n",
       "  'bF1': Array(2.783328e-06, dtype=float32),\n",
       "  'bF2': Array(6.059769e-08, dtype=float32),\n",
       "  'bO': Array(1.295508e-06, dtype=float32),\n",
       "  'bU': Array(5.294611e-08, dtype=float32),\n",
       "  'c0': Array(0.00393636, dtype=float32),\n",
       "  'h0': Array(1.3756433e-06, dtype=float32),\n",
       "  'wC': Array(8.250055e-06, dtype=float32),\n",
       "  'wEM': Array(0.00015424, dtype=float32),\n",
       "  'wF1': Array(0.00015612, dtype=float32),\n",
       "  'wF2': Array(3.3674626e-06, dtype=float32),\n",
       "  'wO': Array(8.082898e-05, dtype=float32),\n",
       "  'wU': Array(2.9702658e-06, dtype=float32)},\n",
       " {'bC': Array(5.3860526e-07, dtype=float32),\n",
       "  'bF1': Array(2.6429737e-08, dtype=float32),\n",
       "  'bF2': Array(4.2145675e-06, dtype=float32),\n",
       "  'bO': Array(0.01078398, dtype=float32),\n",
       "  'bU': Array(5.7055804e-05, dtype=float32),\n",
       "  'bY1': Array(0.10217474, dtype=float32),\n",
       "  'c0': Array(0.01917071, dtype=float32),\n",
       "  'h0': Array(3.621238e-07, dtype=float32),\n",
       "  'wC': Array(2.8099414e-06, dtype=float32),\n",
       "  'wF1': Array(1.7541853e-07, dtype=float32),\n",
       "  'wF2': Array(0.00011994, dtype=float32),\n",
       "  'wO': Array(0.05174534, dtype=float32),\n",
       "  'wU': Array(0.00034987, dtype=float32),\n",
       "  'wY1': Array(0.1735527, dtype=float32)}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(jnp.linalg.norm, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_PARAMS: 1183262\n",
      "DTYPE: float32\n",
      "TOTAL_MEGABYTES: 4.733048\n"
     ]
    }
   ],
   "source": [
    "getsize = lambda s: s.size\n",
    "sizes = jax.tree_util.tree_map(getsize, grads)\n",
    "total_params = 0\n",
    "for layer in sizes:\n",
    "  for _, v in layer.items():\n",
    "    total_params += v\n",
    "\n",
    "print(f\"TOTAL_PARAMS: {total_params}\")\n",
    "print(f\"DTYPE: {grads[0]['bC'].dtype}\")\n",
    "print(f\"TOTAL_MEGABYTES: {total_params*4/1_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.profiler\n",
    "jax.profiler.save_device_memory_profile('test.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 25\n",
      " [[960 961 962 963 964 965 966 967 968 969]\n",
      " [970 971 972 973 974 975 976 977 978 979]\n",
      " [980 981 982 983 984 985 986 987 988 989]\n",
      " [990 991 992 993 994 995 996 997 998 999]] \n",
      "\n",
      " [[970]\n",
      " [980]\n",
      " [990]]\n"
     ]
    }
   ],
   "source": [
    "data = jnp.arange(1000)\n",
    "seqlen = 10\n",
    "bs = 4\n",
    "steps = len(data) // (bs*seqlen)\n",
    "idx = 24\n",
    "data_idx = idx*seqlen*bs\n",
    "next_data_idx = (idx+1)*seqlen*bs\n",
    "print(\n",
    "      f\"steps: {steps}\\n\",\n",
    "      data[data_idx:next_data_idx].reshape(-1, seqlen),\n",
    "      '\\n\\n',\n",
    "      data[data_idx+seqlen:next_data_idx+1:seqlen].reshape(-1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[67, 63],\n",
       "       [80,  1],\n",
       "       [71, 75],\n",
       "       [78, 77],\n",
       "       [81, 81]], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtokens_batch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([63,  1, 75, 77, 81], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytokens_batch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
