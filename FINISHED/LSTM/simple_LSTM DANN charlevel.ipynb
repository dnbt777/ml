{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "from tokenizers import CharBPETokenizer\n",
    "import functools\n",
    "import time\n",
    "\n",
    "\n",
    "gpu_device = jax.device_get('gpu')[0]\n",
    "cpu_device = jax.device_get('cpu')[0]\n",
    "# LSTM\n",
    "# xs = B, input_size = B, T, C\n",
    "# h = c = y = B, output_size = B, T, logits_size = B, T, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 84\n",
      "removed: ü´°…¥ùòÇÊàëùó™·¥áüòéüëÄùóºüò§üìà Ä·¥õ#ùó≤{ ü‚Äôü§£üëåùòÅ…™üöÄü§¶ü§∑~ùó∂ úùòÄ|ùóø`·¥èÔ∏èüéâüí™‚ÄçüòÅüò≠üòâ$üëçÂÄëüç∞ùó±[Âêßüåë}*]ùóØüòÜ‚Äùùó∞ùóµ‚Äúüß†ü§îüò¢‚ôÇ^·¥°ùóª·¥Ñ·¥ò·¥ÄËµ∞üìâü§Ø‚ò†\n",
      "dog [59 70 62] dog\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "with open('data/dnbt_posts.txt', 'r') as file:\n",
    "  dataset = file.read()\n",
    "\n",
    "removed_chars = []\n",
    "frequencies = []\n",
    "for c in set(dataset):\n",
    "  frequencies.append((dataset.count(c), c, c.isalnum()))\n",
    "  if dataset.count(c) < 50:\n",
    "    removed_chars.append(c)\n",
    "    dataset = dataset.replace(c, '')\n",
    "\n",
    "\n",
    "# tokenize\n",
    "vocab = sorted(list(set(dataset)))\n",
    "print(\"vocab length:\", len(vocab))\n",
    "\n",
    "token_to_char = dict(enumerate(vocab))\n",
    "char_to_token = dict([(v, k) for k, v in token_to_char.items()])\n",
    "decode = lambda tokens: \"\".join([token_to_char[int(token)] for token in tokens])\n",
    "encode = lambda chars: jnp.array([char_to_token[c] for c in chars])\n",
    "\n",
    "dataset_tokens = encode(dataset)\n",
    "split_ratio = 0.9\n",
    "train_tokens = dataset_tokens[:int(len(dataset_tokens)*split_ratio)]\n",
    "test_tokens = dataset_tokens[int(len(dataset_tokens)*split_ratio):]\n",
    "del dataset\n",
    "del dataset_tokens\n",
    "\n",
    "\n",
    "print(\"removed:\", \"\".join(removed_chars))\n",
    "print(\"dog\", encode(\"dog\"), decode(encode(\"dog\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '…™', True),\n",
       " (1, '…¥', True),\n",
       " (1, ' ú', True),\n",
       " (1, '·¥ò', True),\n",
       " (1, '·¥õ', True),\n",
       " (1, '‚ò†', False),\n",
       " (1, 'ÂÄë', True),\n",
       " (1, 'Âêß', True),\n",
       " (1, 'Êàë', True),\n",
       " (1, 'Ëµ∞', True),\n",
       " (1, 'ùó™', True),\n",
       " (1, 'ùóØ', True),\n",
       " (1, 'ùó∞', True),\n",
       " (1, 'ùó±', True),\n",
       " (1, 'ùó∂', True),\n",
       " (1, 'ùóø', True),\n",
       " (1, 'ùòÇ', True),\n",
       " (1, 'üåë', False),\n",
       " (1, 'üç∞', False),\n",
       " (1, 'üéâ', False),\n",
       " (1, 'üëÄ', False),\n",
       " (1, 'üìà', False),\n",
       " (1, 'üìâ', False),\n",
       " (1, 'üòÅ', False),\n",
       " (1, 'üòâ', False),\n",
       " (1, 'üò¢', False),\n",
       " (1, 'üò§', False),\n",
       " (1, 'üöÄ', False),\n",
       " (1, 'ü§¶', False),\n",
       " (1, 'ü§Ø', False),\n",
       " (1, 'ü§∑', False),\n",
       " (1, 'üß†', False),\n",
       " (2, '|', False),\n",
       " (2, ' Ä', True),\n",
       " (2, ' ü', True),\n",
       " (2, '·¥è', True),\n",
       " (2, '\\u200d', False),\n",
       " (2, '‚ôÇ', False),\n",
       " (2, 'ùóµ', True),\n",
       " (2, 'ùóª', True),\n",
       " (2, 'ùóº', True),\n",
       " (2, 'ùòÄ', True),\n",
       " (2, 'ùòÅ', True),\n",
       " (2, 'üòÜ', False),\n",
       " (3, '·¥Ä', True),\n",
       " (3, '·¥Ñ', True),\n",
       " (3, '·¥°', True),\n",
       " (3, '‚Äô', False),\n",
       " (3, '‚Äú', False),\n",
       " (3, '‚Äù', False),\n",
       " (3, 'Ô∏è', False),\n",
       " (3, 'ùó≤', True),\n",
       " (3, 'üëå', False),\n",
       " (3, 'üëç', False),\n",
       " (3, 'üò≠', False),\n",
       " (3, 'ü§£', False),\n",
       " (4, '}', False),\n",
       " (4, 'üòé', False),\n",
       " (4, 'ü§î', False),\n",
       " (6, '{', False),\n",
       " (6, '·¥á', True),\n",
       " (6, 'üí™', False),\n",
       " (12, '[', False),\n",
       " (12, ']', False),\n",
       " (20, '~', False),\n",
       " (24, '\\U0001fae1', False),\n",
       " (31, '*', False),\n",
       " (34, '^', False),\n",
       " (37, '$', False),\n",
       " (40, '`', False),\n",
       " (49, '#', False),\n",
       " (96, '%', False),\n",
       " (105, '=', False),\n",
       " (135, 'X', True),\n",
       " (138, '+', False),\n",
       " (145, 'Q', True),\n",
       " (171, 'Z', True),\n",
       " (175, '‚Ä¶', False),\n",
       " (187, '&', False),\n",
       " (194, ';', False),\n",
       " (209, 'J', True),\n",
       " (229, '8', True),\n",
       " (250, 'K', True),\n",
       " (286, 'V', True),\n",
       " (297, 'U', True),\n",
       " (315, '6', True),\n",
       " (325, 'F', True),\n",
       " (334, '9', True),\n",
       " (378, '-', False),\n",
       " (384, \"'\", False),\n",
       " (410, '5', True),\n",
       " (410, 'Y', True),\n",
       " (417, 'D', True),\n",
       " (434, '3', True),\n",
       " (436, '!', False),\n",
       " (444, '4', True),\n",
       " (459, 'W', True),\n",
       " (465, '\"', False),\n",
       " (474, 'G', True),\n",
       " (477, 'H', True),\n",
       " (480, 'q', True),\n",
       " (501, 'C', True),\n",
       " (503, 'R', True),\n",
       " (516, '?', False),\n",
       " (531, 'N', True),\n",
       " (546, '7', True),\n",
       " (568, '(', False),\n",
       " (589, ')', False),\n",
       " (627, 'P', True),\n",
       " (643, '2', True),\n",
       " (670, 'E', True),\n",
       " (682, 'L', True),\n",
       " (810, 'O', True),\n",
       " (822, 'S', True),\n",
       " (898, 'M', True),\n",
       " (960, '1', True),\n",
       " (1040, '_', False),\n",
       " (1120, 'z', True),\n",
       " (1124, 'B', True),\n",
       " (1173, 'j', True),\n",
       " (1219, 'A', True),\n",
       " (1304, 'T', True),\n",
       " (1320, '0', True),\n",
       " (1452, 'I', True),\n",
       " (1660, 'x', True),\n",
       " (2170, ',', False),\n",
       " (2459, '.', False),\n",
       " (2854, '/', False),\n",
       " (3812, 'v', True),\n",
       " (3916, 'üõë', False),\n",
       " (4470, 'k', True),\n",
       " (4567, '@', False),\n",
       " (5029, ':', False),\n",
       " (6053, 'b', True),\n",
       " (6701, 'w', True),\n",
       " (7380, 'f', True),\n",
       " (9323, 'g', True),\n",
       " (10381, 'm', True),\n",
       " (10829, 'c', True),\n",
       " (12176, 'u', True),\n",
       " (12812, 'd', True),\n",
       " (12825, 'y', True),\n",
       " (13082, 'p', True),\n",
       " (14943, 'h', True),\n",
       " (19799, 'l', True),\n",
       " (22950, 'r', True),\n",
       " (23399, 'n', True),\n",
       " (23707, 's', True),\n",
       " (25692, 'i', True),\n",
       " (26141, 'a', True),\n",
       " (28892, '\\n', False),\n",
       " (30301, 'o', True),\n",
       " (34031, 't', True),\n",
       " (42329, 'e', True),\n",
       " (75840, ' ', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm network & other functions\n",
    "def init_LSTM_params(key, lstm_layers, input_size, model_size, output_size):\n",
    "  param_sets = 8 # manual, idc\n",
    "  keys = random.split(key, param_sets*lstm_layers + 2)\n",
    "  hxconcat_size = model_size + model_size\n",
    "  he = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / shape[0])\n",
    "  # supposedly xavier is better for networks using tanh\n",
    "  xavier = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / (shape[0] + shape[1]))\n",
    "  params = [\n",
    "    {\n",
    "      \"wU\" : xavier(keys[param_sets*i + 0], (hxconcat_size, model_size)),\n",
    "      \"bU\" : jnp.zeros((model_size,)),\n",
    "      \"wC\" : xavier(keys[param_sets*i + 6], (hxconcat_size, model_size)),\n",
    "      \"bC\" : jnp.zeros((model_size,)),\n",
    "      \"wF\": xavier(keys[param_sets*i + 1], (hxconcat_size, model_size)),\n",
    "      \"bF\": jnp.zeros((model_size,)),\n",
    "      \"wO\" : xavier(keys[param_sets*i + 3], (hxconcat_size, model_size)),\n",
    "      \"bO\" : jnp.zeros((model_size,)),\n",
    "      \"h0\" : jnp.zeros((model_size,)),\n",
    "      \"c0\" : jnp.zeros((model_size,)),\n",
    "      #\"h0\" : random.normal(keys[param_sets*i + 4], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "      #\"c0\" : random.normal(keys[param_sets*i + 5], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "    }\n",
    "    for i in range(lstm_layers)\n",
    "  ]\n",
    "  params[0].update(\n",
    "    {\n",
    "    # then embedding table weight and bias\n",
    "    \"wEM\" : xavier(keys[param_sets*(param_sets - 1) + 2], (input_size, model_size)),\n",
    "    \"bEM\" : jnp.zeros((model_size,)),\n",
    "\n",
    "  })\n",
    "  params[-1].update(\n",
    "    {\n",
    "      # this is for the y layer, which i am probably imlementing wrong.\n",
    "      \"wY1\" : xavier(keys[param_sets*(lstm_layers-1) + 4], (model_size, model_size)),\n",
    "      \"bY1\" : jnp.zeros((model_size,)),\n",
    "      \"wY2\" : xavier(keys[param_sets*(lstm_layers-1) + 5], (model_size, output_size)),\n",
    "      \"bY2\" : jnp.zeros((output_size,)),\n",
    "    }\n",
    "  )\n",
    "  return params\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def dropout(dropout_key, original_tensor, dropout_rate):\n",
    "  # generate random of same shape\n",
    "  dropout_probs = random.uniform(dropout_key, shape=original_tensor.shape)\n",
    "  # mask = random < dropout_rate\n",
    "  mask = (dropout_probs > dropout_rate) / (1 - dropout_rate) # scale to keep avg the same\n",
    "  return original_tensor * mask\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[]) # static dropout rate?\n",
    "def lstm_step(step_dropout_key, lstm_layer_params, layer_h, layer_c, current_xt, dropout_rate):\n",
    "  hxconcat = jax.lax.concatenate([layer_h, current_xt], dimension=1) #B, h ++ B, C => B, h+c\n",
    "  # update gate\n",
    "  forget_gate = jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wF\"] + lstm_layer_params[\"bF\"])\n",
    "  #update = dropout(step_dropout_keys[0], update, dropout_rate)\n",
    "\n",
    "  # forget\n",
    "  layer_c = layer_c * forget_gate\n",
    "\n",
    "  input_node = jax.nn.tanh(hxconcat @ lstm_layer_params[\"wC\"] + lstm_layer_params[\"bC\"])\n",
    "  #candidate = dropout(step_dropout_keys[1], candidate, dropout_rate)\n",
    "  update = jax.nn.sigmoid(\n",
    "              hxconcat @ lstm_layer_params[\"wU\"] + lstm_layer_params[\"bU\"]\n",
    "            )\n",
    "  input_gate =  update * input_node\n",
    "\n",
    "  # update\n",
    "  layer_c = layer_c + input_gate\n",
    "\n",
    "  # output\n",
    "  layer_h = jax.nn.tanh(layer_c) * jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wO\"] + lstm_layer_params[\"bO\"]) # (B, model_size)\n",
    "\n",
    "  next_layer_xt = dropout(step_dropout_key, layer_h, dropout_rate) # the next layer's input x is the current layer's hidden state\n",
    "  # karpathy: dropout after EACH LAYER not several times in the block. lol.\n",
    "\n",
    "  # i may also need to do dropout horizontally (i.e. dropout the hidden state memory each block)\n",
    "\n",
    "  return (layer_h, layer_c), next_layer_xt\n",
    "\n",
    "\n",
    "# LSTM forward\n",
    "import functools\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate):\n",
    "  batches = xembeds_batch.shape[0]\n",
    "  lstm_layers = len(lstm_params)\n",
    "  model_size = lstm_params[0][\"h0\"].size\n",
    "  # initialize h and c as random/learnable params\n",
    "  #h = jnp.tile(lstm_params[0][\"h0\"], (batches, lstm_layers, 1)) # B, lstm_layer, h_size\n",
    "  #c = jnp.tile(lstm_params[0][\"c0\"], (batches, lstm_layers, 1)) # B, lstm_layer, c_size\n",
    "  # wait.. these are the same for all of the layers.. maybe they shouldn't be\n",
    "  T = xembeds_batch.shape[1]\n",
    "  # take xembeds_batch and pass each xt through the same SINGULAR block. don't update the weight layer. there is only one layer.\n",
    "  dropout_keys = random.split(dropout_key, lstm_layers)\n",
    "\n",
    "  # for each layer:\n",
    "    # scan over xt\n",
    "    # carry : h, c\n",
    "    # a: xt\n",
    "    # b: h,c\n",
    "    # f = lambda ((h, c), xt) : lstm_step(h, c, xt, everything else) => h, c\n",
    "    # scans over xt\n",
    "    # for next layer: xt = h of previous layer. h = h0 and c = c0\n",
    "  \n",
    "  current_embeddings_batch = jnp.transpose(xembeds_batch, (1, 0, 2)) # B, T, C => T, B, C\n",
    "    # The reason for this is that jax.lax.scan only uses the leading dim. why? idk. its dumb, it needs an axis arg so i can scan over whatever\n",
    "\n",
    "  for lstm_layer in range(lstm_layers):\n",
    "    h = jnp.tile(lstm_params[lstm_layer][\"h0\"], (batches, 1))\n",
    "    c = jnp.tile(lstm_params[lstm_layer][\"c0\"], (batches, 1))\n",
    "    # zeroes makes the backprop faster\n",
    "    #h = jnp.zeros((batches, model_size))\n",
    "    #c = jnp.zeros((batches, model_size))\n",
    "    layer_dropout_key = dropout_keys[lstm_layer] # it doesnt matter if this is the same across all layers\n",
    "    # scan should be inexpensive since layer size is small while t size is usually LARGE\n",
    "    # scan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\n",
    "    # scan :: scanfunc -> h_and_c -> xs -> (h_and_c_final, hs_to_be_used_as_input_xt_in_next_layer)\n",
    "    # scanfunc :: (c -> a -> (c, b))\n",
    "    scanfunc = lambda hc, xt : lstm_step(layer_dropout_key, lstm_params[lstm_layer], hc[0], hc[1], xt, dropout_rate)\n",
    "      # for xs: scan along the t dimension! it scans along B by default\n",
    "      # to fix this, we transpose xs with jnp.transpose(current_embeddings_batch, (1, 0, 2))\n",
    "    current_embeddings_batch = jax.lax.scan(scanfunc, (h, c), current_embeddings_batch)[1] # (c, [b]) => [b] ==> B, T, C\n",
    "  \n",
    "\n",
    "  # finally turn current_embeddings_batch into ys (logits)\n",
    "  hs = jnp.transpose(current_embeddings_batch, (1, 0, 2)) # T, B, C => B, T, C\n",
    "  ys = jax.nn.relu(hs @ lstm_params[-1]['wY1'] + lstm_params[-1][\"bY1\"]) # B, T, model_size => B, T, vocab_size\n",
    "  ys = ys @ lstm_params[-1]['wY2'] + lstm_params[-1][\"bY2\"]\n",
    "  return ys\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def loss_func(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def loss_and_value(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  predictions = jnp.argmax(logprobs, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss, predictions\n",
    "\n",
    "\n",
    "jitted_backwards_loss = jax.jit(jax.value_and_grad(loss_func, argnums=1), static_argnames=[])\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['vocab_size'])\n",
    "def embed(lstm_params, xtokens, vocab_size=len(vocab)):\n",
    "  xs_one_hot = jax.nn.one_hot(xtokens, vocab_size, axis=-1) #B, T, vocab_size\n",
    "  activations = xs_one_hot @ lstm_params[0][\"wEM\"] + lstm_params[0][\"bEM\"]\n",
    "  return activations\n",
    "\n",
    "# make optimizer a static arg in jit or it breaks\n",
    "@functools.partial(jax.jit, static_argnames=[\"optimizer\"])\n",
    "def train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer):\n",
    "  step_loss, grads = jitted_backwards_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, lstm_params)\n",
    "  updated_lstm_params = optax.apply_updates(lstm_params, param_updates) \n",
    "  return updated_lstm_params, updated_opt_state, step_loss, grads\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laddering params\n",
    "lstm_layers = 3\n",
    "model_size = 512# 512\n",
    "\n",
    "# laddering\n",
    "# memorize, generalize, move up in complexity. memorize, generalize, move up in complexity.\n",
    "# until desired complexity is reached\n",
    "\n",
    "# strategy: aim for extremely low error*epochs\n",
    "SEQ_LEN      = [2,    4,    10,   25,   50,   50,   50,   50,   ]\n",
    "LR           = [2e-2, 2e-2, 2e-2, 2e-2, 2e-2, 1e-2, 5e-3, 2e-3,]\n",
    "DROPOUT_RATE = [0.00, 0.00, 0.00, 0.1,  0.20, 0.20, 0.20, 0.20]\n",
    "EPOCHS       = [1,    1,    1,    1,    1,    1,    1,    1,  ]\n",
    "rungs = list(zip(SEQ_LEN, LR, DROPOUT_RATE, EPOCHS)) # causes problems if generator\n",
    "\n",
    "# test epoch params\n",
    "# use this to find the optimal LR for laddering steps. could be automated but whatever\n",
    "target_rung = 2\n",
    "sequence_length, lr, dropout_rate, test_epochs = rungs[target_rung]\n",
    "resume_checkpoint = False # saves the checkpoint for rung-1, and reruns rung $rung. this was you dont have to re-climb every rung\n",
    "\n",
    "print_every = 1000\n",
    "train_batch_size = 100\n",
    "val_batch_size = 200\n",
    "\n",
    "# loss: 2.2663 || val_loss: 1.6317 val_acc: 0.5529 || 5 epochs total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (2) | \"ep\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 0/1234 || samples/sec: 17840 || loss: 5.0435 || val_loss: 4.9277 val_acc: 0.1700 || LR = 0.020000\n",
      "TARGET (2) | \"um\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 1000/1234 || samples/sec: 14780 || loss: 2.7572 || val_loss: 2.9164 val_acc: 0.2325 || LR = 0.020000\n",
      "TARGET (4) | \"eply\"\n",
      "PRED   (4) | \"epue\"\n",
      "r,e,s | 1/8, 0/1, 0/739 || samples/sec: 19693 || loss: 2.7359 || val_loss: 2.6774 val_acc: 0.2637 || LR = 0.020000\n",
      "TARGET (10) | \"eply: @tri\"\n",
      "PRED   (10) | \"eply: @aue\"\n",
      "r,e,s | 2/8, 0/1, 0/335 || samples/sec: 7177 || loss: 2.6325 || val_loss: 2.3668 val_acc: 0.3500 || LR = 0.020000\n",
      "ended after rung 2\n"
     ]
    }
   ],
   "source": [
    "# run test epoch\n",
    "\n",
    "# setup vars\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "losses = []\n",
    "start = time.time()\n",
    "\n",
    "# train\n",
    "# train rungs\n",
    "for r, rung in enumerate(rungs):\n",
    "  if resume_checkpoint and r < target_rung:\n",
    "    # skip rungs until the target rung\n",
    "    continue\n",
    "\n",
    "  if r > target_rung:\n",
    "    print(f\"ended after rung {r-1}\")\n",
    "    break\n",
    "  \n",
    "  if resume_checkpoint and r == target_rung:\n",
    "    lstm_params = lstm_params_checkpoint\n",
    "\n",
    "  sequence_length, lr, dropout_rate, epochs = rung\n",
    "  # initialize if first rung\n",
    "  if r == 0:\n",
    "    lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "    optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "    opt_state = optimizer.init(lstm_params)\n",
    "  else:\n",
    "    opt_state.hyperparams['learning_rate'] = lr\n",
    "\n",
    "  # train\n",
    "  for epoch in range(epochs):\n",
    "      steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "      for step in range(steps): # probably wrong but w/e\n",
    "        # train\n",
    "        # B, T where T = sequence_length\n",
    "        train_data_idx = step*sequence_length*train_batch_size\n",
    "        next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "        xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "        ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "        dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "        lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "        losses.append(step_loss)\n",
    "\n",
    "        if ((epoch*step + step) % print_every == 0) or (epoch + steps == 0):\n",
    "          end = time.time()\n",
    "          duration = end - start\n",
    "          # train inference example (no dropout)\n",
    "          xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "          last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "          prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "          # val batch\n",
    "          j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "          val_idx = j*val_batch_size*sequence_length\n",
    "          next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "          xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "          ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "          \n",
    "          val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "          val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "          # print train status\n",
    "          x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "          y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "          yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "          #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "          lines = [\n",
    "            f'TARGET ({len(y)}) | \"{y}\"',\n",
    "            f'PRED   ({len(yhat)}) | \"{yhat}\"',\n",
    "            f\"r,e,s | {r}/{len(rungs)}, {epoch}/{epochs}, {step}/{steps} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || \"\n",
    "            f\"loss: {sum(losses)/len(losses):1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || \" \n",
    "            f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          start = time.time()\n",
    "  if r < target_rung:\n",
    "    # stop saving checkpoint after training the target rung\n",
    "    lstm_params_checkpoint = lstm_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new rung 0\n",
      "TARGET (2) | \"ep\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 0/1234 || samples/sec: 3394 || loss: 5.0435 || val_loss: 4.9277 val_acc: 0.1700 || LR = 0.020000\n",
      "TARGET (2) | \"ep\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 100/1234 || samples/sec: 16252 || loss: 3.2576 || val_loss: 3.4519 val_acc: 0.2000 || LR = 0.020000\n",
      "TARGET (2) | \"sc\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 200/1234 || samples/sec: 16144 || loss: 3.0758 || val_loss: 2.6926 val_acc: 0.2325 || LR = 0.020000\n",
      "TARGET (2) | \"nt\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 300/1234 || samples/sec: 15518 || loss: 2.9792 || val_loss: 2.6449 val_acc: 0.2400 || LR = 0.020000\n",
      "TARGET (2) | \"ik\"\n",
      "PRED   (2) | \"yn\"\n",
      "r,e,s | 0/8, 0/1, 400/1234 || samples/sec: 18901 || loss: 2.9016 || val_loss: 2.7475 val_acc: 0.2875 || LR = 0.020000\n",
      "TARGET (2) | \"  \"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 500/1234 || samples/sec: 16580 || loss: 2.8631 || val_loss: 2.6574 val_acc: 0.2925 || LR = 0.020000\n",
      "TARGET (2) | \"ce\"\n",
      "PRED   (2) | \"io\"\n",
      "r,e,s | 0/8, 0/1, 600/1234 || samples/sec: 19732 || loss: 2.8318 || val_loss: 2.6309 val_acc: 0.2625 || LR = 0.020000\n",
      "TARGET (2) | \"r \"\n",
      "PRED   (2) | \"t \"\n",
      "r,e,s | 0/8, 0/1, 700/1234 || samples/sec: 17938 || loss: 2.8106 || val_loss: 2.5344 val_acc: 0.3125 || LR = 0.020000\n",
      "TARGET (2) | \"ad\"\n",
      "PRED   (2) | \"et\"\n",
      "r,e,s | 0/8, 0/1, 800/1234 || samples/sec: 17296 || loss: 2.7898 || val_loss: 2.3705 val_acc: 0.3725 || LR = 0.020000\n",
      "TARGET (2) | \"a \"\n",
      "PRED   (2) | \"tn\"\n",
      "r,e,s | 0/8, 0/1, 900/1234 || samples/sec: 19206 || loss: 2.7729 || val_loss: 2.8531 val_acc: 0.2775 || LR = 0.020000\n",
      "TARGET (2) | \"um\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 1000/1234 || samples/sec: 18457 || loss: 2.7572 || val_loss: 2.9164 val_acc: 0.2325 || LR = 0.020000\n",
      "TARGET (2) | \"ly\"\n",
      "PRED   (2) | \"ly\"\n",
      "r,e,s | 0/8, 0/1, 1100/1234 || samples/sec: 19145 || loss: 2.7474 || val_loss: 2.5498 val_acc: 0.2925 || LR = 0.020000\n",
      "TARGET (2) | \"re\"\n",
      "PRED   (2) | \"n \"\n",
      "r,e,s | 0/8, 0/1, 1200/1234 || samples/sec: 14029 || loss: 2.7390 || val_loss: 2.5747 val_acc: 0.3375 || LR = 0.020000\n",
      "new rung 1\n",
      "TARGET (4) | \"eply\"\n",
      "PRED   (4) | \"epue\"\n",
      "r,e,s | 1/8, 0/1, 0/739 || samples/sec: 3414 || loss: 2.7359 || val_loss: 2.6774 val_acc: 0.2637 || LR = 0.020000\n",
      "TARGET (4) | \"scri\"\n",
      "PRED   (4) | \"   o\"\n",
      "r,e,s | 1/8, 0/1, 100/739 || samples/sec: 2869 || loss: 2.7210 || val_loss: 2.4939 val_acc: 0.3237 || LR = 0.020000\n",
      "TARGET (4) | \"ike \"\n",
      "PRED   (4) | \"yne \"\n",
      "r,e,s | 1/8, 0/1, 200/739 || samples/sec: 8850 || loss: 2.7046 || val_loss: 2.4320 val_acc: 0.3287 || LR = 0.020000\n",
      "TARGET (4) | \"cert\"\n",
      "PRED   (4) | \"tor \"\n",
      "r,e,s | 1/8, 0/1, 300/739 || samples/sec: 10106 || loss: 2.6903 || val_loss: 2.5155 val_acc: 0.3250 || LR = 0.020000\n",
      "TARGET (4) | \"ade \"\n",
      "PRED   (4) | \"ene \"\n",
      "r,e,s | 1/8, 0/1, 400/739 || samples/sec: 9793 || loss: 2.6772 || val_loss: 2.3128 val_acc: 0.3637 || LR = 0.020000\n",
      "TARGET (4) | \"umbe\"\n",
      "PRED   (4) | \" tee\"\n",
      "r,e,s | 1/8, 0/1, 500/739 || samples/sec: 10242 || loss: 2.6622 || val_loss: 2.4757 val_acc: 0.3212 || LR = 0.020000\n",
      "TARGET (4) | \"re, \"\n",
      "PRED   (4) | \"u   \"\n",
      "r,e,s | 1/8, 0/1, 600/739 || samples/sec: 10773 || loss: 2.6494 || val_loss: 2.3292 val_acc: 0.3462 || LR = 0.020000\n",
      "TARGET (4) | \"hess\"\n",
      "PRED   (4) | \"o   \"\n",
      "r,e,s | 1/8, 0/1, 700/739 || samples/sec: 10826 || loss: 2.6380 || val_loss: 2.2231 val_acc: 0.3775 || LR = 0.020000\n",
      "new rung 2\n",
      "TARGET (10) | \"eply: @tri\"\n",
      "PRED   (10) | \"eply: @aue\"\n",
      "r,e,s | 2/8, 0/1, 0/335 || samples/sec: 3427 || loss: 2.6325 || val_loss: 2.3668 val_acc: 0.3500 || LR = 0.020000\n",
      "TARGET (10) | \"    reply:\"\n",
      "PRED   (10) | \"     eply:\"\n",
      "r,e,s | 2/8, 0/1, 100/335 || samples/sec: 1935 || loss: 2.6150 || val_loss: 2.4232 val_acc: 0.3595 || LR = 0.020000\n",
      "TARGET (10) | \"umbers 1-1\"\n",
      "PRED   (10) | \" seer  t0 \"\n",
      "r,e,s | 2/8, 0/1, 200/335 || samples/sec: 3742 || loss: 2.5970 || val_loss: 2.1697 val_acc: 0.3895 || LR = 0.020000\n",
      "TARGET (10) | \"long line?\"\n",
      "PRED   (10) | \"toog tong \"\n",
      "r,e,s | 2/8, 0/1, 300/335 || samples/sec: 4092 || loss: 2.5793 || val_loss: 2.1895 val_acc: 0.4065 || LR = 0.020000\n",
      "new rung 3\n",
      "TARGET (25) | \"eply: @trickylabyrinth A \"\n",
      "PRED   (25) | \"eply: @leanhe yrea ngeewh\"\n",
      "r,e,s | 3/8, 0/1, 0/140 || samples/sec: 2250 || loss: 2.5733 || val_loss: 2.1076 val_acc: 0.4178 || LR = 0.020000\n",
      "TARGET (25) | \"far Used to think it was \"\n",
      "PRED   (25) | \"tone o   th theng tn tin \"\n",
      "r,e,s | 3/8, 0/1, 100/140 || samples/sec: 1141 || loss: 2.5571 || val_loss: 2.0725 val_acc: 0.4260 || LR = 0.020000\n",
      "new rung 4\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @leotk  lcl  ng et @etter tare stneeenh tire\"\n",
      "r,e,s | 4/8, 0/1, 0/70 || samples/sec: 1825 || loss: 2.5503 || val_loss: 1.9748 val_acc: 0.4532 || LR = 0.020000\n",
      "new rung 5\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @yeetke enl  ng eth@e ter tare st ee n  tire\"\n",
      "r,e,s | 5/8, 0/1, 0/70 || samples/sec: 950 || loss: 2.5398 || val_loss: 1.9481 val_acc: 0.4586 || LR = 0.010000\n",
      "new rung 6\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @yeatk  ynl_ ng et ie tir tare  t ee n  tire\"\n",
      "r,e,s | 6/8, 0/1, 0/70 || samples/sec: 1142 || loss: 2.5280 || val_loss: 1.9247 val_acc: 0.4641 || LR = 0.005000\n",
      "new rung 7\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @aeatk  ynl  ngeet @e ter tare  t ee n  tise\"\n",
      "r,e,s | 7/8, 0/1, 0/70 || samples/sec: 1045 || loss: 2.5158 || val_loss: 1.9084 val_acc: 0.4711 || LR = 0.002000\n"
     ]
    }
   ],
   "source": [
    "# train (laddering)\n",
    "\n",
    "\n",
    "# init some parameters\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "train_batch_size = 100\n",
    "val_batch_size = 200\n",
    "print_every = 100\n",
    "j = 0\n",
    "losses = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# train rungs\n",
    "for r, rung in enumerate(rungs):\n",
    "  print(f\"new rung {r}\")\n",
    "  sequence_length, lr, dropout_rate, epochs = rung\n",
    "  # initialize if first rung\n",
    "  if r == 0:\n",
    "    lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "    optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "    opt_state = optimizer.init(lstm_params)\n",
    "  else:\n",
    "    opt_state.hyperparams['learning_rate'] = lr\n",
    "\n",
    "  # train\n",
    "  for epoch in range(epochs):\n",
    "      steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "      for step in range(steps): # probably wrong but w/e\n",
    "        # train\n",
    "        # B, T where T = sequence_length\n",
    "        train_data_idx = step*sequence_length*train_batch_size\n",
    "        next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "        xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "        ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "        dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "        lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "        losses.append(step_loss)\n",
    "\n",
    "        if ((epoch*step + step) % print_every == 0) or (epoch + steps == 0):\n",
    "          end = time.time()\n",
    "          duration = end - start\n",
    "          # train inference example (no dropout)\n",
    "          xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "          last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "          prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "          # val batch\n",
    "          j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "          val_idx = j*val_batch_size*sequence_length\n",
    "          next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "          xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "          ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "          \n",
    "          val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "          val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "          # print train status\n",
    "          x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "          y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "          yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "          #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "          lines = [\n",
    "            f'TARGET ({len(y)}) | \"{y}\"',\n",
    "            f'PRED   ({len(yhat)}) | \"{yhat}\"',\n",
    "            f\"r,e,s | {r}/{len(rungs)}, {epoch}/{epochs}, {step}/{steps} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || \"\n",
    "            f\"loss: {sum(losses)/len(losses):1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || \" \n",
    "            f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "# train engine parameters\n",
    "\n",
    "# this is the function that takes the current hyperparameters\n",
    "# and makes candidate ones to test.\n",
    "#@jax.jit\n",
    "from itertools import product\n",
    "from jax import random as jrand\n",
    "def make_candidates(key, current_sequence_length, current_learning_rate, current_dropout_rate):\n",
    "  # get lr candidates\n",
    "  upscale = jnp.array([1, 2, 10, 100], dtype=jnp.float32)\n",
    "  downscale = 1.0 / upscale # 8x, 4x, 2x, 1x, 0.5x, 0.25x, etc\n",
    "  scale = jnp.concatenate([upscale, downscale])\n",
    "  lr_candidates = current_learning_rate * scale\n",
    "\n",
    "  # get seq length candidates (this does have an effect)\n",
    "  #sequence_length_candidates = jnp.array(list(set([current_sequence_length, 2, 4, 8, 15, 25, 50, 100])))\n",
    "  sequence_length_candidates = jnp.array([current_sequence_length]) # dont change this\n",
    "\n",
    "  # future: dropout\n",
    "  dropout_candidates = jnp.array(list(set([0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])))\n",
    "\n",
    "  # get all possible combinations as a generator\n",
    "  # randomly shuffle by preshuffling the inputs\n",
    "  keys = jrand.split(key, 10)\n",
    "  candidates = product(\n",
    "    jrand.permutation(keys[0], sequence_length_candidates, independent=True),\n",
    "    jrand.permutation(keys[1], lr_candidates, independent=True),\n",
    "    jrand.permutation(keys[2], dropout_candidates, independent=True),\n",
    "  )\n",
    "\n",
    "  return candidates\n",
    "\n",
    "\n",
    "## PARAMETERS ##\n",
    "\n",
    "# every n epochs, do another hyperparameter search\n",
    "# lookahead k steps or epochs.\n",
    "# pick the best set of hyperparameters and do the next n epochs with them. repeat\n",
    "# in the future, update whenever final_loss < 0.95*start_loss\n",
    "retune_min_epochs = 20 # how many epochs minimum to train for before rechecking hyperparameters. set super high for normal h tuning\n",
    "lookahead_steps = 30 # steps to train to test candidate hyperparameters\n",
    "candidate_eval_func = lambda final_loss: -final_loss # in this case, candidates are evaluated higher if their final loss is low\n",
    "\n",
    "initial_lr = 3e-4\n",
    "\n",
    "\n",
    "# model params\n",
    "lstm_layers = 2\n",
    "model_size = 512\n",
    "initial_sequence_length = 100\n",
    "initial_dropout_rate = 0.4 # for now\n",
    "\n",
    "candidate_limit = 100\n",
    "\n",
    "\n",
    "# general testing params\n",
    "epochs = 10000\n",
    "print_every = 100\n",
    "train_batch_size = 100\n",
    "val_batch_size = 200\n",
    "\n",
    "test_key = jrand.PRNGKey(1203)\n",
    "print(len(list(make_candidates(test_key, initial_lr, initial_sequence_length, 0.1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retuning: initial hyperparameter tuning\n",
      "this will be slow as functions are jitted\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "old: todo\n",
      "new: (Array(100, dtype=int32), Array(0.00294, dtype=float32), Array(0.8, dtype=float32)) => -3.318880081176758\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e toetot  teoeeee te teee toe  tot toete   ee teaeeetee tooeee      reply: @ooo  n     @oee    to t \"\n",
      "e:0/10000 s:33/34 || samples/sec: 14312 || loss: 2.8495 || val_loss: 3.2569 val_acc: 0.2062 || LR = 0.002940\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  tt ton  tntet eean tene ao   ton aersl   et tnton te  antor       reply: @aoteen   B @n   e  tt ne\"\n",
      "e:1/10000 s:33/34 || samples/sec: 1316590 || loss: 2.5645 || val_loss: 2.5969 val_acc: 0.2580 || LR = 0.002940\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e tnttonh tntet eetn tete to t ton aetrr   et tntol teu tntol       reply: @aaleinhBAB @0   e  tt le\"\n",
      "e:2/10000 s:33/34 || samples/sec: 1502124 || loss: 2.4211 || val_loss: 2.3734 val_acc: 0.2912 || LR = 0.002881\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e tn tonh t tett  tn tet  to t tonhaearrt het tnton teu tnton       reply: @aaceenABAP @n   er tntne\"\n",
      "e:3/10000 s:33/34 || samples/sec: 1502851 || loss: 2.3408 || val_loss: 2.2511 val_acc: 0.3138 || LR = 0.002881\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e tnethnh t tette tf tet  to t ton tealnt het tnton tot t ton       reply: @aaciegABAP @    er tntle\"\n",
      "e:4/10000 s:33/34 || samples/sec: 1379675 || loss: 2.2625 || val_loss: 2.1551 val_acc: 0.3319 || LR = 0.002824\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t tn thnh tntetde tf tatd to t ton teaert tet t ton tot t ton       reply: @seswigABAP @ d  er tntne\"\n",
      "e:5/10000 s:33/34 || samples/sec: 1299801 || loss: 2.1907 || val_loss: 2.0692 val_acc: 0.3475 || LR = 0.002824\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t tn thth t tetdt tf tanl to t ton teaert tot t ton tot t ton       reply: @aadwigABAP @ d  er tnele\"\n",
      "e:6/10000 s:33/34 || samples/sec: 1247597 || loss: 2.1242 || val_loss: 1.9969 val_acc: 0.3613 || LR = 0.002767\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t tn thth t tetdt tf tavleto t ton teaert tet t tor tot t tor       reply: @sedwigABAP @ dther tnere\"\n",
      "e:7/10000 s:33/34 || samples/sec: 1283248 || loss: 2.0883 || val_loss: 1.9339 val_acc: 0.3736 || LR = 0.002767\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t ts thth t tetdt tf tavleto t ton teaert tot t ton tot t ton      rreply: @sudwigABAP @ dther txpre\"\n",
      "e:8/10000 s:33/34 || samples/sec: 1310093 || loss: 2.0527 || val_loss: 1.8830 val_acc: 0.3848 || LR = 0.002712\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  tn thth t tetdt tf tavleto t ton teaert tot t ton tot tnton       reply: @yudwigABAP @ dther tnpne\"\n",
      "e:9/10000 s:33/34 || samples/sec: 1322915 || loss: 1.9920 || val_loss: 1.8391 val_acc: 0.3948 || LR = 0.002712\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t ts thth t tetdt tf tavl to t ton teaert iot t tonttot tntont      reply: @sudwigABAP @ dther tnpnp\"\n",
      "e:10/10000 s:33/34 || samples/sec: 1318841 || loss: 1.9560 || val_loss: 1.7992 val_acc: 0.4041 || LR = 0.002658\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t tetde tf tavl to t ton teaert iot t tonttot tntont      reply: @yudwigABAP @ duher tapnp\"\n",
      "e:11/10000 s:33/34 || samples/sec: 1319024 || loss: 1.9322 || val_loss: 1.7648 val_acc: 0.4125 || LR = 0.002658\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t ts thth t tetdt tf tavl to t ton teaent iot tntonttot tntont      reply: @yudwigABAP @ dther tnpmp\"\n",
      "e:12/10000 s:33/34 || samples/sec: 1286323 || loss: 1.8937 || val_loss: 1.7375 val_acc: 0.4202 || LR = 0.002604\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setde tf tavd to t ton teaent iot t sonttot tntont      reply: @audwigABAP @ dther tnpmp\"\n",
      "e:13/10000 s:33/34 || samples/sec: 1293653 || loss: 1.8658 || val_loss: 1.7130 val_acc: 0.4274 || LR = 0.002604\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thlh t setdt tf tavl to t ton teaert iot t sonttot tnsont      reply: @sudwigABAP @ dwher tnpmp\"\n",
      "e:14/10000 s:33/34 || samples/sec: 1284574 || loss: 1.8311 || val_loss: 1.6925 val_acc: 0.4339 || LR = 0.002552\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setde tf tavl to t ton teaert iot t son tot t son       reply: @sudwigABAP @ dther tnpmp\"\n",
      "e:15/10000 s:33/34 || samples/sec: 1223070 || loss: 1.8295 || val_loss: 1.6738 val_acc: 0.4399 || LR = 0.002552\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdt tf tavl to t ton teaert iot t son tot tnson       reply: @yudwigABAP @ dther tnpmp\"\n",
      "e:16/10000 s:33/34 || samples/sec: 1318255 || loss: 1.8062 || val_loss: 1.6577 val_acc: 0.4455 || LR = 0.002501\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl to e ton teaert iot t son tot tnson       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:17/10000 s:33/34 || samples/sec: 1361355 || loss: 1.7929 || val_loss: 1.6408 val_acc: 0.4507 || LR = 0.002501\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl to e ton tearrt iot t sonttot t sont      reply: @sudwigABAP @ duher tnpmp\"\n",
      "e:18/10000 s:33/34 || samples/sec: 1480909 || loss: 1.7679 || val_loss: 1.6287 val_acc: 0.4556 || LR = 0.002451\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setde tf tavd to e ton tearst iot t sonttot tnsont      reply: @ludwigABAP @nduher tnpmp\"\n",
      "e:19/10000 s:33/34 || samples/sec: 1489037 || loss: 1.7556 || val_loss: 1.6157 val_acc: 0.4602 || LR = 0.002451\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd to e tov teaest iot t son tot t son       reply: @sudwigABAP @ duher tnpmp\"\n",
      "e:20/10000 s:33/34 || samples/sec: 1412992 || loss: 1.7382 || val_loss: 1.6037 val_acc: 0.4645 || LR = 0.002402\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl to e tov tearrt iot t son tot t son       reply: @yudwigABAP @ duher tnpmp\"\n",
      "e:21/10000 s:33/34 || samples/sec: 1339841 || loss: 1.7168 || val_loss: 1.5919 val_acc: 0.4685 || LR = 0.002402\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov tearrt iot t son tot tnson       reply: @sudwigABAP @ndther inpmp\"\n",
      "e:22/10000 s:33/34 || samples/sec: 1329694 || loss: 1.7011 || val_loss: 1.5838 val_acc: 0.4724 || LR = 0.002354\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov teaert iot t sor tot t sor       reply: @sudwigABAP @ dther inpmp\"\n",
      "e:23/10000 s:33/34 || samples/sec: 1443883 || loss: 1.7018 || val_loss: 1.5765 val_acc: 0.4759 || LR = 0.002354\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov teaert iot t son tot t son       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:24/10000 s:33/34 || samples/sec: 1343216 || loss: 1.6711 || val_loss: 1.5677 val_acc: 0.4794 || LR = 0.002307\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh af tavd tone tov tearrt iot t son tot t sor       reply: @sudwigABAP @ dther tnpmp\"\n",
      "e:25/10000 s:33/34 || samples/sec: 1311771 || loss: 1.6656 || val_loss: 1.5599 val_acc: 0.4826 || LR = 0.002307\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl tone tov teaert iot t sor tot t sor       reply: @ludwigABAP @ dther tnpmp\"\n",
      "e:26/10000 s:33/34 || samples/sec: 1345496 || loss: 1.6490 || val_loss: 1.5537 val_acc: 0.4857 || LR = 0.002261\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavd tone tov teaert iot t sor tot t sor       reply: @sudwigABAP @ dther inpmp\"\n",
      "e:27/10000 s:33/34 || samples/sec: 1352392 || loss: 1.6342 || val_loss: 1.5494 val_acc: 0.4886 || LR = 0.002261\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov teaert iot t sos tot t sos       reply: @yudwigABAP i dther inpmp\"\n",
      "e:28/10000 s:33/34 || samples/sec: 1351901 || loss: 1.6346 || val_loss: 1.5438 val_acc: 0.4914 || LR = 0.002216\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh af tavp tone tov teaeri iot t son tot t sor       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:29/10000 s:33/34 || samples/sec: 1348724 || loss: 1.6256 || val_loss: 1.5400 val_acc: 0.4940 || LR = 0.002216\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavf tone tov teaest iot t son tot t son       reply: @sudwigABAP @ dther inpmp\"\n",
      "e:30/10000 s:33/34 || samples/sec: 1347755 || loss: 1.6086 || val_loss: 1.5364 val_acc: 0.4965 || LR = 0.002171\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov teaess iot t sos tot t sos       reply: @sudwigABAP @ldther tnpmp\"\n",
      "e:31/10000 s:33/34 || samples/sec: 1347123 || loss: 1.5982 || val_loss: 1.5311 val_acc: 0.4989 || LR = 0.002171\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavd tone tov teaert iot t sos tot t sos       reply: @sudwigABAP @ dther txpmp\"\n",
      "e:32/10000 s:33/34 || samples/sec: 1355113 || loss: 1.6076 || val_loss: 1.5302 val_acc: 0.5012 || LR = 0.002128\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavd tone tov tearrt iot t sos tot t sos       reply: @sudwigABAP @ dther txpmp\"\n",
      "e:33/10000 s:33/34 || samples/sec: 1338885 || loss: 1.5698 || val_loss: 1.5256 val_acc: 0.5034 || LR = 0.002128\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov tearrt iot t sor tot t sor       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:34/10000 s:33/34 || samples/sec: 1338453 || loss: 1.5773 || val_loss: 1.5243 val_acc: 0.5056 || LR = 0.002085\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov tearss iot t sos tot t sos       reply: @yudwigABAP @ndther tnpmp\"\n",
      "e:35/10000 s:33/34 || samples/sec: 1347395 || loss: 1.5810 || val_loss: 1.5225 val_acc: 0.5076 || LR = 0.002085\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavd tone tov teaers iot t sos tot t sos       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:36/10000 s:33/34 || samples/sec: 1319880 || loss: 1.5587 || val_loss: 1.5216 val_acc: 0.5095 || LR = 0.002044\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov teaass iot t sos tot t sos       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:37/10000 s:33/34 || samples/sec: 1309352 || loss: 1.5367 || val_loss: 1.5207 val_acc: 0.5114 || LR = 0.002044\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov tearss iot t sos tot t sos       reply: @sudwigABAP @ndther tnamp\"\n",
      "e:38/10000 s:33/34 || samples/sec: 1302621 || loss: 1.5437 || val_loss: 1.5196 val_acc: 0.5132 || LR = 0.002003\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone ton tearr  iot t sor tot t sor       reply: @ludwigABAP @ndther tnpmp\"\n",
      "e:39/10000 s:33/34 || samples/sec: 1316164 || loss: 1.5488 || val_loss: 1.5187 val_acc: 0.5149 || LR = 0.002003\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov tears  iot t sos tot t sor       reply: @ludwigABAP @ndther txpmp\"\n",
      "e:40/10000 s:33/34 || samples/sec: 1407322 || loss: 1.5126 || val_loss: 1.5174 val_acc: 0.5165 || LR = 0.001963\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts ahth t setch of tavf tone ion oeaas  iot t sos tot a sos       reply: @ludwigABAP @n ther ixpmp\"\n",
      "e:41/10000 s:33/34 || samples/sec: 1455323 || loss: 1.5096 || val_loss: 1.5175 val_acc: 0.5181 || LR = 0.001963\n",
      "retuning: \n",
      "val_error: 1.5175 !< 0.9997*1.5174\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "old: todo\n",
      "new: (Array(100, dtype=int32), Array(0.00019235, dtype=float32), Array(0.8, dtype=float32)) => -1.5133428573608398\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov tearr  iot t sos tot t sor       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:42/10000 s:33/34 || samples/sec: 14877 || loss: 1.4960 || val_loss: 1.5097 val_acc: 0.5196 || LR = 0.000192\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:43/10000 s:33/34 || samples/sec: 1337515 || loss: 1.4916 || val_loss: 1.5058 val_acc: 0.5212 || LR = 0.000192\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:44/10000 s:33/34 || samples/sec: 1341831 || loss: 1.4665 || val_loss: 1.5062 val_acc: 0.5226 || LR = 0.000189\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:45/10000 s:33/34 || samples/sec: 1308042 || loss: 1.4706 || val_loss: 1.5069 val_acc: 0.5241 || LR = 0.000189\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:46/10000 s:33/34 || samples/sec: 1313825 || loss: 1.4618 || val_loss: 1.5079 val_acc: 0.5254 || LR = 0.000185\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:47/10000 s:33/34 || samples/sec: 1323480 || loss: 1.4682 || val_loss: 1.5091 val_acc: 0.5267 || LR = 0.000185\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:48/10000 s:33/34 || samples/sec: 1287792 || loss: 1.4577 || val_loss: 1.5104 val_acc: 0.5280 || LR = 0.000181\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:49/10000 s:33/34 || samples/sec: 1260930 || loss: 1.4493 || val_loss: 1.5109 val_acc: 0.5292 || LR = 0.000181\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:50/10000 s:33/34 || samples/sec: 1219425 || loss: 1.4575 || val_loss: 1.5116 val_acc: 0.5303 || LR = 0.000177\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:51/10000 s:33/34 || samples/sec: 1405315 || loss: 1.4557 || val_loss: 1.5133 val_acc: 0.5314 || LR = 0.000177\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:52/10000 s:33/34 || samples/sec: 1386925 || loss: 1.4420 || val_loss: 1.5143 val_acc: 0.5325 || LR = 0.000174\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:53/10000 s:33/34 || samples/sec: 1395540 || loss: 1.4453 || val_loss: 1.5147 val_acc: 0.5335 || LR = 0.000174\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:54/10000 s:33/34 || samples/sec: 1331416 || loss: 1.4386 || val_loss: 1.5149 val_acc: 0.5345 || LR = 0.000170\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:55/10000 s:33/34 || samples/sec: 1305469 || loss: 1.4354 || val_loss: 1.5157 val_acc: 0.5355 || LR = 0.000170\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:56/10000 s:33/34 || samples/sec: 1328723 || loss: 1.4388 || val_loss: 1.5178 val_acc: 0.5364 || LR = 0.000167\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:57/10000 s:33/34 || samples/sec: 1316326 || loss: 1.4372 || val_loss: 1.5188 val_acc: 0.5373 || LR = 0.000167\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:58/10000 s:33/34 || samples/sec: 1348590 || loss: 1.4402 || val_loss: 1.5199 val_acc: 0.5381 || LR = 0.000164\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:59/10000 s:33/34 || samples/sec: 1326316 || loss: 1.4364 || val_loss: 1.5201 val_acc: 0.5389 || LR = 0.000164\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:60/10000 s:33/34 || samples/sec: 1340550 || loss: 1.4283 || val_loss: 1.5217 val_acc: 0.5397 || LR = 0.000160\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:61/10000 s:33/34 || samples/sec: 1362782 || loss: 1.4276 || val_loss: 1.5218 val_acc: 0.5405 || LR = 0.000160\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fanüõë      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:62/10000 s:33/34 || samples/sec: 1412575 || loss: 1.4213 || val_loss: 1.5237 val_acc: 0.5412 || LR = 0.000157\n",
      "retuning: \n",
      "val_error: 1.5237 !< 0.9997*1.5218\n",
      "0\n",
      "10\n",
      "20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m train_data_idx \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[1;32m     69\u001b[0m next_train_data_idx \u001b[38;5;241m=\u001b[39m (step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_train_data_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_tokens):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     72\u001b[0m xtokens_batch \u001b[38;5;241m=\u001b[39m train_tokens[train_data_idx:next_train_data_idx]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length) \u001b[38;5;66;03m#(B, T)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/array.py:294\u001b[0m, in \u001b[0;36mArrayImpl.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    293\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_bool_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/array.py:628\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train w retuning\n",
    "\n",
    "# init some parameters\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "train_batch_size = 100\n",
    "val_batch_size = train_batch_size\n",
    "print_every = 100_000\n",
    "j = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# init state\n",
    "lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=initial_lr)\n",
    "opt_state = optimizer.init(lstm_params)\n",
    "sequence_length = initial_sequence_length\n",
    "dropout_rate = initial_dropout_rate\n",
    "\n",
    "previous_hyperparameters = (sequence_length, initial_lr, dropout_rate)\n",
    "\n",
    "\n",
    "retune = True\n",
    "retune_msg = \"initial hyperparameter tuning\\nthis will be slow as functions are jitted\"\n",
    "\n",
    "\n",
    "decay = 0.98\n",
    "decay_epochs = 2\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    if epoch % decay_epochs == 0:\n",
    "       opt_state.hyperparams['learning_rate'] = opt_state.hyperparams['learning_rate'] * decay\n",
    "    # retune hyperparameters\n",
    "    if retune: # includes the first epoch\n",
    "      epochs_since_retune = 0\n",
    "      print(\"retuning:\", retune_msg)\n",
    "      # just do lr for now\n",
    "      current_lr = opt_state.hyperparams['learning_rate']\n",
    "      current_sequence_length = sequence_length\n",
    "      current_dropout_rate = dropout_rate\n",
    "      candidate_key = jrand.PRNGKey(int(time.time()))\n",
    "      candidates = make_candidates(candidate_key, current_sequence_length, current_lr, current_dropout_rate) # candidate 'moves'\n",
    "      best_candidate = (None, -100000) # candidate, score\n",
    "      i = 0\n",
    "      for candidate in [previous_hyperparameters] + list(candidates):\n",
    "        if i == candidate_limit:\n",
    "           break\n",
    "        if i % 10 == 0: print(i)\n",
    "        i += 1\n",
    "        # make copies of the current params and opt state, and train with them for a few steps\n",
    "        candidate_params = lstm_params\n",
    "        candidate_opt_state = opt_state\n",
    "        # update copies with candidate hyperparams:\n",
    "        sequence_length = candidate[0]\n",
    "        candidate_opt_state.hyperparams['learning_rate'] = candidate[1]\n",
    "        dropout_rate = candidate[2]\n",
    "        # for now, just eval future positions on losses[-1].\n",
    "        # future evals can be whatever. average accuracy over a val set is a good one.\n",
    "        candidate_val_losses = []\n",
    "        for step in range(lookahead_steps):\n",
    "          # train each candidate hyperparam set on the exact same data\n",
    "          train_data_idx = step*sequence_length*train_batch_size\n",
    "          next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "          if next_train_data_idx > len(train_tokens):\n",
    "              break\n",
    "          xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "          ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "          dropout_key = random.PRNGKey(epoch*lookahead_steps + step) # unique for every step\n",
    "          # train\n",
    "          candidate_params, candidate_opt_state, step_loss, _ = train(\n",
    "            dropout_key, candidate_params, xtokens_batch, ytokens_batch, candidate_opt_state, dropout_rate, optimizer\n",
    "          )\n",
    "\n",
    "          # get val accuracy after training\n",
    "          j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "          val_idx = j*val_batch_size*sequence_length\n",
    "          next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "          xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "          ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "          \n",
    "          val_loss, prediction_val_batch = loss_and_value(dropout_key, candidate_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "          val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "          candidate_val_losses.append(val_loss)\n",
    "\n",
    "        candidate_result = -sum(candidate_val_losses)/len(candidate_val_losses)#val_loss # just do train loss for now. the best is probably accuracy though.\n",
    "        candidate_score = candidate_result#candidate_eval_func(candidate_result)\n",
    "        if candidate_score > best_candidate[1]: # replace the current winner if this one scores better\n",
    "          best_candidate = (candidate, candidate_score)\n",
    "        \n",
    "        candidate_val_losses = []\n",
    "\n",
    "      # finally, update to the winner\n",
    "      print(f\"old: todo\")\n",
    "      print(f\"new: {best_candidate[0]} => {best_candidate[1]}\")\n",
    "\n",
    "      ## update hyperparams:\n",
    "      new_hyperparameters = best_candidate[0]\n",
    "      previous_hyperparameters = new_hyperparameters\n",
    "      new_sequence_length, new_lr, new_dropout_rate = new_hyperparameters\n",
    "      opt_state.hyperparams['learning_rate'] = new_lr\n",
    "      sequence_length = int(new_sequence_length)\n",
    "      dropout_rate = new_dropout_rate\n",
    "\n",
    "      retune = False\n",
    "\n",
    "\n",
    "    # do regular training with the current hyperparameters for $sprint_distance epochs\n",
    "    # retesting hyperparameters every $sprint_distance is handled by the if block above\n",
    "    steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "    for step in range(steps): # probably wrong but w/e\n",
    "      # train\n",
    "      # B, T where T = sequence_length\n",
    "      train_data_idx = step*sequence_length*train_batch_size\n",
    "      next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "      xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "      ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "      dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "      lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "      losses.append(step_loss)\n",
    "\n",
    "      # val\n",
    "      j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      val_idx = j*val_batch_size*sequence_length\n",
    "      next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "      \n",
    "      val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      val_losses.append(val_loss)\n",
    "      val_accuracies.append(val_accuracy)\n",
    "\n",
    "      if (step == steps - 1):\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        # train inference example (no dropout)\n",
    "        xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "        last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "        prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "        # print train status\n",
    "        x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "        y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "        yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "        #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "        avg_loss = sum(losses)/len(losses)\n",
    "        avg_val_loss = sum(val_losses)/len(val_losses)\n",
    "        avg_val_acc = sum(val_accuracies)/len(val_accuracies)\n",
    "        lines = [\n",
    "          f'TARGET | \"{y}\"',\n",
    "          f'PRED   | \"{yhat}\"',\n",
    "          f\"e:{epoch}/{epochs} s:{step}/{steps} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || \"\n",
    "          f\"loss: {step_loss:1.4f} || val_loss: {avg_val_loss:1.4f} val_acc: {avg_val_acc:1.4f} || \" \n",
    "          f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "        ]\n",
    "        print(\"\\n\".join(lines))\n",
    "        start = time.time()\n",
    "    \n",
    "    epochs_since_retune += 1\n",
    "    # if the val error hasn't decreased to 90%, try to retune hyperparameters\n",
    "    target_decrease = 0.9997\n",
    "    if epoch > 0 and epochs_since_retune > retune_min_epochs and avg_val_loss > previous_epoch_val_loss*target_decrease:\n",
    "       retune = True\n",
    "       retune_msg = f\"\\nval_error: {avg_val_loss:0.4f} !< {target_decrease:0.4f}*{previous_epoch_val_loss:0.4f}\"\n",
    "    previous_epoch_val_loss = avg_val_loss\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal train parameters\n",
    "\n",
    "## PARAMETERS ##\n",
    "lr = 0.0007\n",
    "sequence_length = 100\n",
    "dropout_rate = 0.25\n",
    "\n",
    "\n",
    "decay_lr = False\n",
    "decay = 0.98\n",
    "decay_epochs = 3\n",
    "\n",
    "\n",
    "# model params\n",
    "lstm_layers = 2\n",
    "model_size = 1024\n",
    "\n",
    "resume_train_state = False\n",
    "\n",
    "# general testing params\n",
    "epochs = 10000\n",
    "print_every = 100_000\n",
    "train_batch_size = 46\n",
    "val_batch_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"te              epl:::                                                                epl:::        \"\n",
      "e:1/10000 s:100/100 || samples/sec: 541 || loss: 3.0564 || val_loss: 3.4078 val_acc: 0.1562 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"to en n        reply: @anleone  tne to  n   toen to   tn  toen to  to toen  n        reply: @aonlnee\"\n",
      "e:2/10000 s:100/100 || samples/sec: 1413 || loss: 2.6120 || val_loss: 2.7691 val_acc: 0.2176 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaen ng       reply: @andaene  tne to  ne  then th   tn  toen to  to theng ng       reply: @aonlnee\"\n",
      "e:3/10000 s:100/100 || samples/sec: 1366 || loss: 2.4226 || val_loss: 2.4638 val_acc: 0.2563 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaat ng       reply: @andaene  @ne ton ne  then th   tn  toat tor te theng ng       reply: @aanlnee\"\n",
      "e:4/10000 s:100/100 || samples/sec: 1383 || loss: 2.3146 || val_loss: 2.3181 val_acc: 0.2839 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaal ng       reply: @sndaenl  @ne tor ne  then th   tn  toat tor ta theng ng       reply: @saclrae\"\n",
      "e:5/10000 s:100/100 || samples/sec: 1519 || loss: 2.2208 || val_loss: 2.2162 val_acc: 0.3053 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teael ng       reply: @sndienl  @ne tot oe  then th   tn  toat tor tortheng ng       reply: @saclree\"\n",
      "e:6/10000 s:100/100 || samples/sec: 1521 || loss: 2.1477 || val_loss: 2.1382 val_acc: 0.3225 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndiinl  @te tet oe  then th   tn  toet tor tartheng ng       reply: @saclree\"\n",
      "e:7/10000 s:100/100 || samples/sec: 1520 || loss: 2.0973 || val_loss: 2.0713 val_acc: 0.3373 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndiinl  @ne tot oe  then th   tn  toet tot ta theng ng       reply: @suclrde\"\n",
      "e:8/10000 s:100/100 || samples/sec: 1408 || loss: 2.0487 || val_loss: 2.0147 val_acc: 0.3505 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaerkng       reply: @sndiicl7 @ e tot oe  then th   tt  aoet tot ta theng ng       reply: @luclrde\"\n",
      "e:9/10000 s:100/100 || samples/sec: 1671 || loss: 1.9940 || val_loss: 1.9661 val_acc: 0.3621 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaerkng       reply: @lndeill7 @ e tot oe  thes th   tn  toet tot ta thesg ng       reply: @luclrde\"\n",
      "e:10/10000 s:100/100 || samples/sec: 1716 || loss: 1.9558 || val_loss: 1.9226 val_acc: 0.3724 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndrell7 @ e tot oe  thes th   tn  toet tot ta theng ng       reply: @sucirde\"\n",
      "e:11/10000 s:100/100 || samples/sec: 1727 || loss: 1.9224 || val_loss: 1.8851 val_acc: 0.3818 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndrel_7 @ e tot oe  thes th   tn  toet tot ta thesg ng       reply: @sucirde\"\n",
      "e:12/10000 s:100/100 || samples/sec: 1547 || loss: 1.8775 || val_loss: 1.8514 val_acc: 0.3904 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrol07 @ e tot oe  thes th   tn  aoet tot ta thesg ng       reply: @lucirde\"\n",
      "e:13/10000 s:100/100 || samples/sec: 1549 || loss: 1.8586 || val_loss: 1.8212 val_acc: 0.3984 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrul07 @ e tot oe  thes ih   tn  ahat too ta thesg ng       reply: @sucerde\"\n",
      "e:14/10000 s:100/100 || samples/sec: 1557 || loss: 1.8240 || val_loss: 1.7956 val_acc: 0.4057 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @lndrul07 @ e tot oe  thes ih   tn  aoat too ta theng ng       reply: @lucerde\"\n",
      "e:15/10000 s:100/100 || samples/sec: 1557 || loss: 1.8038 || val_loss: 1.7719 val_acc: 0.4124 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrul07 @ e tot oe  thes ih   an  ahet too ta thesk ng       reply: @sucerde\"\n",
      "e:16/10000 s:100/100 || samples/sec: 1541 || loss: 1.7837 || val_loss: 1.7511 val_acc: 0.4187 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeteog       reply: @andrul07 @ e tot ce  thes ih   an  ahet too ta thenk ng       reply: @aucerde\"\n",
      "e:17/10000 s:100/100 || samples/sec: 1536 || loss: 1.7626 || val_loss: 1.7319 val_acc: 0.4245 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeteog       reply: @lndrul07 @ e tot ce  thes ih   an  ahet too ta thenk ng       reply: @lunerde\"\n",
      "e:18/10000 s:100/100 || samples/sec: 1562 || loss: 1.7405 || val_loss: 1.7143 val_acc: 0.4299 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrul07 @ e tot ca  thes ih   an  ahet too ta thenk ng       reply: @suberde\"\n",
      "e:19/10000 s:100/100 || samples/sec: 1558 || loss: 1.7328 || val_loss: 1.6987 val_acc: 0.4349 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  ahet too ta thenk ng       reply: @yuberde\"\n",
      "e:20/10000 s:100/100 || samples/sec: 1543 || loss: 1.7038 || val_loss: 1.6838 val_acc: 0.4397 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  ahet too ta thesk ng       reply: @yuberde\"\n",
      "e:21/10000 s:100/100 || samples/sec: 1675 || loss: 1.7041 || val_loss: 1.6702 val_acc: 0.4441 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  aaet too ta thesk ng       reply: @yuberde\"\n",
      "e:22/10000 s:100/100 || samples/sec: 1726 || loss: 1.6691 || val_loss: 1.6571 val_acc: 0.4483 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  ahet too ta theng ng       reply: @yuberde\"\n",
      "e:23/10000 s:100/100 || samples/sec: 1503 || loss: 1.6694 || val_loss: 1.6453 val_acc: 0.4523 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @sndkul07 @ e tot ce  thes ih   an  ahet too ta toesk ng       reply: @suberde\"\n",
      "e:24/10000 s:100/100 || samples/sec: 1557 || loss: 1.6459 || val_loss: 1.6342 val_acc: 0.4561 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndkul07 @ e tot ce  toes ih   an  ahat too ta toenk ng       reply: @yuberde\"\n",
      "e:25/10000 s:100/100 || samples/sec: 1565 || loss: 1.6444 || val_loss: 1.6242 val_acc: 0.4597 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndkul07 @ e tot ce  toes ih   sn  ahat too ta toesg ng       reply: @yuberde\"\n",
      "e:26/10000 s:100/100 || samples/sec: 1486 || loss: 1.6489 || val_loss: 1.6146 val_acc: 0.4631 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @yndkul07 @ e tot ce  toes ih   an  ahat too ta toesg ng       reply: @yuberde\"\n",
      "e:27/10000 s:100/100 || samples/sec: 1616 || loss: 1.6312 || val_loss: 1.6057 val_acc: 0.4663 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @sndkul07 @ e sot ce  toes io   an  ahat too ta toesg ng       reply: @suberde\"\n",
      "e:28/10000 s:100/100 || samples/sec: 1683 || loss: 1.6188 || val_loss: 1.5973 val_acc: 0.4694 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetitg       reply: @sndkul07 @ e sot ce  toes ih   sn  ahat too ta thesk ng       reply: @suberde\"\n",
      "e:29/10000 s:100/100 || samples/sec: 1747 || loss: 1.6119 || val_loss: 1.5894 val_acc: 0.4723 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih   an  ahat too ta toesk ng       reply: @suberde\"\n",
      "e:30/10000 s:100/100 || samples/sec: 1657 || loss: 1.5964 || val_loss: 1.5815 val_acc: 0.4751 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toesk ng       reply: @suberde\"\n",
      "e:31/10000 s:100/100 || samples/sec: 1538 || loss: 1.5853 || val_loss: 1.5745 val_acc: 0.4778 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toenk ng       reply: @suberde\"\n",
      "e:32/10000 s:100/100 || samples/sec: 1703 || loss: 1.5929 || val_loss: 1.5680 val_acc: 0.4804 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaatetg       reply: @sndkul07 @ e sot ce  toes io l an  ahat ioo ta toeng ng       reply: @suberde\"\n",
      "e:33/10000 s:100/100 || samples/sec: 1715 || loss: 1.5720 || val_loss: 1.5622 val_acc: 0.4828 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaatetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toeng ng       reply: @suberde\"\n",
      "e:34/10000 s:100/100 || samples/sec: 1650 || loss: 1.5552 || val_loss: 1.5558 val_acc: 0.4852 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toesk ng       reply: @suberde\"\n",
      "e:35/10000 s:100/100 || samples/sec: 1664 || loss: 1.5650 || val_loss: 1.5501 val_acc: 0.4875 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaatetg       reply: @sndkul07 @ e sot ce  toes io   an  ahat ioo ta toesg ng       reply: @suberde\"\n",
      "e:36/10000 s:100/100 || samples/sec: 1741 || loss: 1.5594 || val_loss: 1.5452 val_acc: 0.4897 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaateng       reply: @sndkul07 @ e sot ce  toes io   an  ahat ioo ta toesk ng       reply: @suberde\"\n",
      "e:37/10000 s:100/100 || samples/sec: 1735 || loss: 1.5565 || val_loss: 1.5405 val_acc: 0.4918 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih   an  ahat ioo ta toesk ng       reply: @suberde\"\n",
      "e:38/10000 s:100/100 || samples/sec: 1680 || loss: 1.5556 || val_loss: 1.5359 val_acc: 0.4939 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih   an  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:39/10000 s:100/100 || samples/sec: 1541 || loss: 1.5439 || val_loss: 1.5314 val_acc: 0.4958 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih l an  ahat ioo ta toesging       reply: @suberde\"\n",
      "e:40/10000 s:100/100 || samples/sec: 1744 || loss: 1.5359 || val_loss: 1.5270 val_acc: 0.4977 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih   an  ahat ioo ta toesking       reply: @suberde\"\n",
      "e:41/10000 s:100/100 || samples/sec: 1747 || loss: 1.5450 || val_loss: 1.5224 val_acc: 0.4995 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  aaat ioo ta toeng ng       reply: @suberde\"\n",
      "e:42/10000 s:100/100 || samples/sec: 1741 || loss: 1.5223 || val_loss: 1.5194 val_acc: 0.5013 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:43/10000 s:100/100 || samples/sec: 1739 || loss: 1.5341 || val_loss: 1.5154 val_acc: 0.5030 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo ta toenk ng       reply: @suberde\"\n",
      "e:44/10000 s:100/100 || samples/sec: 1741 || loss: 1.5138 || val_loss: 1.5120 val_acc: 0.5047 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  aaat ioo ta toenk ng       reply: @suberde\"\n",
      "e:45/10000 s:100/100 || samples/sec: 1745 || loss: 1.5019 || val_loss: 1.5091 val_acc: 0.5063 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:46/10000 s:100/100 || samples/sec: 1739 || loss: 1.5145 || val_loss: 1.5061 val_acc: 0.5078 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenking       reply: @suberde\"\n",
      "e:47/10000 s:100/100 || samples/sec: 1739 || loss: 1.5243 || val_loss: 1.5030 val_acc: 0.5093 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:48/10000 s:100/100 || samples/sec: 1715 || loss: 1.4904 || val_loss: 1.4993 val_acc: 0.5108 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo ta toenk ng       reply: @suberde\"\n",
      "e:49/10000 s:100/100 || samples/sec: 1518 || loss: 1.4893 || val_loss: 1.4966 val_acc: 0.5122 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:50/10000 s:100/100 || samples/sec: 1679 || loss: 1.4984 || val_loss: 1.4940 val_acc: 0.5135 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  ahat ioo ta toenging       reply: @suberde\"\n",
      "e:51/10000 s:100/100 || samples/sec: 1698 || loss: 1.4906 || val_loss: 1.4916 val_acc: 0.5148 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo to toenking       reply: @suberde\"\n",
      "e:52/10000 s:100/100 || samples/sec: 1716 || loss: 1.4975 || val_loss: 1.4885 val_acc: 0.5161 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo to toesging       reply: @suberde\"\n",
      "e:53/10000 s:100/100 || samples/sec: 1615 || loss: 1.4802 || val_loss: 1.4860 val_acc: 0.5174 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  ahat ioo ta ooenging       reply: @suberde\"\n",
      "e:54/10000 s:100/100 || samples/sec: 1487 || loss: 1.4774 || val_loss: 1.4838 val_acc: 0.5186 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l ats aaat ioo to toenging       reply: @suberde\"\n",
      "e:55/10000 s:100/100 || samples/sec: 1526 || loss: 1.4707 || val_loss: 1.4819 val_acc: 0.5198 || LR = 0.000700\n",
      "TARGET | \"relaxingüõë      reply: @angkul07 ive noticed this too, its what got me thinkingüõë      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo ta ooenking       reply: @suberde\"\n",
      "e:56/10000 s:100/100 || samples/sec: 1474 || loss: 1.4826 || val_loss: 1.4796 val_acc: 0.5209 || LR = 0.000700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m ytokens_batch \u001b[38;5;241m=\u001b[39m train_tokens[train_data_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:next_train_data_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length) \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m dropout_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(epoch\u001b[38;5;241m*\u001b[39msteps \u001b[38;5;241m+\u001b[39m step) \u001b[38;5;66;03m# unique for every step\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m lstm_params, opt_state, step_loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(step_loss)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# val\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# normal training\n",
    "# train normally\n",
    "\n",
    "# init some parameters\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# init state\n",
    "if not resume_train_state:\n",
    "  optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "  lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "  opt_state = optimizer.init(lstm_params)\n",
    "else:\n",
    "  opt_state.hyperparams['learning_rate'] = lr\n",
    "\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    if decay_lr and epoch != 0 and epoch % decay_epochs == 0:\n",
    "       opt_state.hyperparams['learning_rate'] = opt_state.hyperparams['learning_rate'] * decay\n",
    "\n",
    "    # train\n",
    "    steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "    for step in range(steps): # probably wrong but w/e\n",
    "      # B, T where T = sequence_length\n",
    "      train_data_idx = step*sequence_length*train_batch_size\n",
    "      next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "      xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "      ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "      dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "      lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "      losses.append(step_loss)\n",
    "\n",
    "      # val\n",
    "      j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      val_idx = j*val_batch_size*sequence_length\n",
    "      next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "      \n",
    "      val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      val_losses.append(val_loss)\n",
    "      val_accuracies.append(val_accuracy)\n",
    "\n",
    "      if (step == steps - 1):\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        # train inference example (no dropout)\n",
    "        xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "        last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "        prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "        # print train status\n",
    "        x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "        y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "        yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "        #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "        avg_loss = sum(losses)/len(losses)\n",
    "        avg_val_loss = sum(val_losses)/len(val_losses)\n",
    "        avg_val_acc = sum(val_accuracies)/len(val_accuracies)\n",
    "        lines = [\n",
    "          f'TARGET | \"{y}\"',\n",
    "          f'PRED   | \"{yhat}\"',\n",
    "          f\"e:{epoch+1}/{epochs} s:{step+1}/{steps} || samples/sec: {train_batch_size*steps/(duration):0.0f} || \"\n",
    "          f\"loss: {step_loss:1.4f} || val_loss: {avg_val_loss:1.4f} val_acc: {avg_val_acc:1.4f} || \" \n",
    "          f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "        ]\n",
    "        print(\"\\n\".join(lines))\n",
    "        start = time.time()\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference settings\n",
    "\n",
    "temperature = 1.5   # from 0 to 2. 1 is normal.\n",
    "\n",
    "reply_prompt = \"reply: \"\n",
    "post_prompt = \"post: \"\n",
    "\n",
    "prompt = post_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reply: bc VA E, = kAHDBl\n",
      "IMa-\n",
      "You'l lodp theys no\n",
      " mil,\n",
      "Apjoa\n",
      "hp 1x, n1L)\n",
      "I can light 16fr  5'06 (ca\n",
      "i builder\n",
      "likely bro\n",
      "289169x23% upsam much-crathik\n",
      "ingy\n",
      "\n",
      "you p4yl Set20but?üõë"
     ]
    }
   ],
   "source": [
    "# run the model!\n",
    "\n",
    "def inference(key, chars, temperature):\n",
    "  xtokens = encode(chars)[None, :]\n",
    "  xembed = embed(lstm_params, xtokens) # artificial single batch\n",
    "  logits = lstm_forward(key, lstm_params, xembed, 0)[0][-1] # logits of the first B and last T in the B T C. should be (C,)\n",
    "  probs = jax.nn.softmax(logits/(temperature + 0.001))\n",
    "  yhattokens = random.choice(key, a=logits.shape[0], p=probs) # no need for axis=-1 since logits are (C,)\n",
    "  return yhattokens\n",
    "\n",
    "\n",
    "steps = 1000\n",
    "import time\n",
    "seed = int(1000*time.time())\n",
    "keys = random.split(random.PRNGKey(seed), steps)\n",
    "text =  \"\\n\"*50 + 'reply: '\n",
    "print(text.replace('\\n\\n', ''), end='')\n",
    "for i in range(steps):\n",
    "  next_token = inference(keys[i], text[-sequence_length:], temperature)\n",
    "  next_char = decode([next_token])[-1]\n",
    "  if next_char == 'üõë':\n",
    "    print(next_char, end='')\n",
    "    break\n",
    "  text += next_char\n",
    "  line_length = 50\n",
    "  if (len(text) - 50) % line_length == 0:\n",
    "    print()\n",
    "  print(next_char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bC': Array(7.5398985e-18, dtype=float32),\n",
       "  'bEM': Array(1.2155308e-16, dtype=float32),\n",
       "  'bF': Array(5.38526e-19, dtype=float32),\n",
       "  'bO': Array(2.959863e-19, dtype=float32),\n",
       "  'bU': Array(7.320699e-19, dtype=float32),\n",
       "  'bY1': Array(4.5822423e-18, dtype=float32),\n",
       "  'bY2': Array(0.01353781, dtype=float32),\n",
       "  'c0': Array(9.196565e-18, dtype=float32),\n",
       "  'h0': Array(0., dtype=float32),\n",
       "  'wC': Array(4.9677064e-16, dtype=float32),\n",
       "  'wEM': Array(1.1980155e-16, dtype=float32),\n",
       "  'wF': Array(2.0176042e-17, dtype=float32),\n",
       "  'wO': Array(1.3135809e-17, dtype=float32),\n",
       "  'wU': Array(2.7428173e-17, dtype=float32),\n",
       "  'wY1': Array(9.819348e-17, dtype=float32),\n",
       "  'wY2': Array(1.5380868e-16, dtype=float32)}]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(jnp.linalg.norm, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsize = lambda s: s.size\n",
    "sizes = jax.tree_util.tree_map(getsize, grads)\n",
    "total_params = 0\n",
    "for layer in sizes:\n",
    "  for _, v in layer.items():\n",
    "    total_params += v\n",
    "\n",
    "print(f\"TOTAL_PARAMS: {total_params}\")\n",
    "print(f\"DTYPE: {grads[0]['bC'].dtype}\")\n",
    "print(f\"TOTAL_MEGABYTES: {total_params*4/1_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.profiler\n",
    "jax.profiler.save_device_memory_profile('test.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.arange(1000)\n",
    "seqlen = 10\n",
    "bs = 4\n",
    "steps = len(data) // (bs*seqlen)\n",
    "idx = 24\n",
    "data_idx = idx*seqlen*bs\n",
    "next_data_idx = (idx+1)*seqlen*bs\n",
    "print(\n",
    "      f\"steps: {steps}\\n\",\n",
    "      data[data_idx:next_data_idx].reshape(-1, seqlen),\n",
    "      '\\n\\n',\n",
    "      data[data_idx+seqlen:next_data_idx+1:seqlen].reshape(-1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
